[
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "PolynomialFeatures",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "OneHotEncoder",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "OneHotEncoder",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "cross_val_score",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "cross_val_score",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "variance_inflation_factor",
        "importPath": "statsmodels.stats.outliers_influence",
        "description": "statsmodels.stats.outliers_influence",
        "isExtraImport": true,
        "detail": "statsmodels.stats.outliers_influence",
        "documentation": {}
    },
    {
        "label": "variance_inflation_factor",
        "importPath": "statsmodels.stats.outliers_influence",
        "description": "statsmodels.stats.outliers_influence",
        "isExtraImport": true,
        "detail": "statsmodels.stats.outliers_influence",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "LassoCV",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "RidgeCV",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "ElasticNetCV",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "LinearRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "Lasso",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "Ridge",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "ElasticNet",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "LinearRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "LassoCV",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "Ridge",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "ElasticNet",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "metrics",
        "importPath": "sklearn",
        "description": "sklearn",
        "isExtraImport": true,
        "detail": "sklearn",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "mean_squared_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "r2_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "nan_inf_janitor",
        "kind": 2,
        "importPath": "scripts.functions",
        "description": "scripts.functions",
        "peekOfCode": "def nan_inf_janitor(df):\n    \"\"\"\n    Cleans a dataframe of NaNs and Infs.\n    Args:\n        df (_type_): _description_\n    Returns:\n        _type_: _description_\n    \"\"\"\n    # This function takes a dataframe and removes the NaNs and Infs from it using Imputation.\n    # if the column contains mostly integer/float values...",
        "detail": "scripts.functions",
        "documentation": {}
    },
    {
        "label": "extract_feature_range",
        "kind": 2,
        "importPath": "scripts.functions",
        "description": "scripts.functions",
        "peekOfCode": "def extract_feature_range(col):\n    # Given a column, return the interquartile range\n    # Determine the 1st and 3rd quartiles\n    q1 = col.quantile(0.25)\n    q3 = col.quantile(0.75)\n    # Determine the IQR\n    iqr = q3 - q1\n    return iqr\ndef detect_squarefootage_feature(col, feature_values):\n    # given a series of values in a column, return True if the values are likely square footage values.",
        "detail": "scripts.functions",
        "documentation": {}
    },
    {
        "label": "detect_squarefootage_feature",
        "kind": 2,
        "importPath": "scripts.functions",
        "description": "scripts.functions",
        "peekOfCode": "def detect_squarefootage_feature(col, feature_values):\n    # given a series of values in a column, return True if the values are likely square footage values.\n    # Determine the 1st and 3rd quartiles\n    likely_sqft_range = range(\n        100, 5000\n    )  # Likely to be sqft if in this range of values.\n    if (\n        feature_values.min() in likely_sqft_range\n        and feature_values.max() in likely_sqft_range\n        or (\"area\" in col.lower() or \"sq\" in col.lower())",
        "detail": "scripts.functions",
        "documentation": {}
    },
    {
        "label": "detect_judgementfeature",
        "kind": 2,
        "importPath": "scripts.functions",
        "description": "scripts.functions",
        "peekOfCode": "def detect_judgementfeature(col, feature_values):\n    # given a series of values in a column, return True if the values are likely judgement features.\n    # Determine the 1st and 3rd quartiles\n    likely_judgement_range = range(\n        1, 11\n    )  # Likely to be judgement if in this range of values. The range will include 1 and 10.\n    # todo --- the range above could cause issues with the left number.\n    if (\n        feature_values.min() in likely_judgement_range\n        and feature_values.max() in likely_judgement_range",
        "detail": "scripts.functions",
        "documentation": {}
    },
    {
        "label": "engineer_features",
        "kind": 2,
        "importPath": "scripts.functions",
        "description": "scripts.functions",
        "peekOfCode": "def engineer_features(df, features=[]):\n    # Given a dataframe, return a dataframe with new features.\n    # Create a new feature called 'Total SF' that is the sum of the '1st Flr SF' and '2nd Flr SF' columns\n    df[\"total_sqft\"] = df[\"1st_flr_sf\"] + df[\"2nd_flr_sf\"]\n    # Create a new feature called 'Total Bath' that is the sum of the 'Full Bath' and 'Half Bath' columns\n    df[\"total_bath\"] = df[\"full_bath\"] + (df[\"half_bath\"] * 0.5)\n    # Create a new feature called 'Total Porch SF' that is the sum of the 'Open Porch SF', 'Enclosed Porch', '3Ssn Porch', and 'Screen Porch' columns\n    df[\"total_porch_sf\"] = (\n        df[\"open_porch_sf\"]\n        + df[\"enclosed_porch\"]",
        "detail": "scripts.functions",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 1 # initialize figure number\ntrain = pd.read_csv('../data/train.csv')\ntest = pd.read_csv('../data/test.csv')\noriginal_train = train.copy() # make a copy of the original data\noriginal_test = test.copy() # keep a copy of the original data\nprint(\"Data loaded from train/test cleaned csv files.\")\nfeatures_before_selection = train.columns\nfeature_count_before_selection = len(features_before_selection) # will be used later to compare the number of features before and after selection\n# I want to immedietely correct the column names to make them easier to work with. snake_case is the standard for python.\ntrain.columns = [str(col).replace(' ','_').lower() for col in train.columns]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train = pd.read_csv('../data/train.csv')\ntest = pd.read_csv('../data/test.csv')\noriginal_train = train.copy() # make a copy of the original data\noriginal_test = test.copy() # keep a copy of the original data\nprint(\"Data loaded from train/test cleaned csv files.\")\nfeatures_before_selection = train.columns\nfeature_count_before_selection = len(features_before_selection) # will be used later to compare the number of features before and after selection\n# I want to immedietely correct the column names to make them easier to work with. snake_case is the standard for python.\ntrain.columns = [str(col).replace(' ','_').lower() for col in train.columns]\noriginal_train.columns = [str(col).replace(' ','_').lower() for col in original_train.columns]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test = pd.read_csv('../data/test.csv')\noriginal_train = train.copy() # make a copy of the original data\noriginal_test = test.copy() # keep a copy of the original data\nprint(\"Data loaded from train/test cleaned csv files.\")\nfeatures_before_selection = train.columns\nfeature_count_before_selection = len(features_before_selection) # will be used later to compare the number of features before and after selection\n# I want to immedietely correct the column names to make them easier to work with. snake_case is the standard for python.\ntrain.columns = [str(col).replace(' ','_').lower() for col in train.columns]\noriginal_train.columns = [str(col).replace(' ','_').lower() for col in original_train.columns]\n# To make sure the columns are the same in both dataframes, I will use the same code for the test dataframe.",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "original_train",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "original_train = train.copy() # make a copy of the original data\noriginal_test = test.copy() # keep a copy of the original data\nprint(\"Data loaded from train/test cleaned csv files.\")\nfeatures_before_selection = train.columns\nfeature_count_before_selection = len(features_before_selection) # will be used later to compare the number of features before and after selection\n# I want to immedietely correct the column names to make them easier to work with. snake_case is the standard for python.\ntrain.columns = [str(col).replace(' ','_').lower() for col in train.columns]\noriginal_train.columns = [str(col).replace(' ','_').lower() for col in original_train.columns]\n# To make sure the columns are the same in both dataframes, I will use the same code for the test dataframe.\ntest.columns = [str(col).replace(' ','_').lower() for col in test.columns]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "original_test",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "original_test = test.copy() # keep a copy of the original data\nprint(\"Data loaded from train/test cleaned csv files.\")\nfeatures_before_selection = train.columns\nfeature_count_before_selection = len(features_before_selection) # will be used later to compare the number of features before and after selection\n# I want to immedietely correct the column names to make them easier to work with. snake_case is the standard for python.\ntrain.columns = [str(col).replace(' ','_').lower() for col in train.columns]\noriginal_train.columns = [str(col).replace(' ','_').lower() for col in original_train.columns]\n# To make sure the columns are the same in both dataframes, I will use the same code for the test dataframe.\ntest.columns = [str(col).replace(' ','_').lower() for col in test.columns]\noriginal_test = [str(col).replace(' ','_').lower() for col in original_test.columns]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "features_before_selection",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "features_before_selection = train.columns\nfeature_count_before_selection = len(features_before_selection) # will be used later to compare the number of features before and after selection\n# I want to immedietely correct the column names to make them easier to work with. snake_case is the standard for python.\ntrain.columns = [str(col).replace(' ','_').lower() for col in train.columns]\noriginal_train.columns = [str(col).replace(' ','_').lower() for col in original_train.columns]\n# To make sure the columns are the same in both dataframes, I will use the same code for the test dataframe.\ntest.columns = [str(col).replace(' ','_').lower() for col in test.columns]\noriginal_test = [str(col).replace(' ','_').lower() for col in original_test.columns]\n# %%\n# Generate the Baseline Model",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "feature_count_before_selection",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "feature_count_before_selection = len(features_before_selection) # will be used later to compare the number of features before and after selection\n# I want to immedietely correct the column names to make them easier to work with. snake_case is the standard for python.\ntrain.columns = [str(col).replace(' ','_').lower() for col in train.columns]\noriginal_train.columns = [str(col).replace(' ','_').lower() for col in original_train.columns]\n# To make sure the columns are the same in both dataframes, I will use the same code for the test dataframe.\ntest.columns = [str(col).replace(' ','_').lower() for col in test.columns]\noriginal_test = [str(col).replace(' ','_').lower() for col in original_test.columns]\n# %%\n# Generate the Baseline Model\n# The baseline model is the model that is used to compare the performance of the other models. It is the simplest model that can be used to predict the target variable. In this case, the baseline model is the mean of the target variable.",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train.columns",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train.columns = [str(col).replace(' ','_').lower() for col in train.columns]\noriginal_train.columns = [str(col).replace(' ','_').lower() for col in original_train.columns]\n# To make sure the columns are the same in both dataframes, I will use the same code for the test dataframe.\ntest.columns = [str(col).replace(' ','_').lower() for col in test.columns]\noriginal_test = [str(col).replace(' ','_').lower() for col in original_test.columns]\n# %%\n# Generate the Baseline Model\n# The baseline model is the model that is used to compare the performance of the other models. It is the simplest model that can be used to predict the target variable. In this case, the baseline model is the mean of the target variable.\n# The baseline model is used to determine if the other models are better than the baseline model. If the other models are better than the baseline model, then the other models are useful. If the other models are not better than the baseline model, then the other models are not useful.\n# Baseline Model",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "original_train.columns",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "original_train.columns = [str(col).replace(' ','_').lower() for col in original_train.columns]\n# To make sure the columns are the same in both dataframes, I will use the same code for the test dataframe.\ntest.columns = [str(col).replace(' ','_').lower() for col in test.columns]\noriginal_test = [str(col).replace(' ','_').lower() for col in original_test.columns]\n# %%\n# Generate the Baseline Model\n# The baseline model is the model that is used to compare the performance of the other models. It is the simplest model that can be used to predict the target variable. In this case, the baseline model is the mean of the target variable.\n# The baseline model is used to determine if the other models are better than the baseline model. If the other models are better than the baseline model, then the other models are useful. If the other models are not better than the baseline model, then the other models are not useful.\n# Baseline Model\nbaseline_model = train['saleprice'].mean() # 181469.70160897123",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test.columns",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test.columns = [str(col).replace(' ','_').lower() for col in test.columns]\noriginal_test = [str(col).replace(' ','_').lower() for col in original_test.columns]\n# %%\n# Generate the Baseline Model\n# The baseline model is the model that is used to compare the performance of the other models. It is the simplest model that can be used to predict the target variable. In this case, the baseline model is the mean of the target variable.\n# The baseline model is used to determine if the other models are better than the baseline model. If the other models are better than the baseline model, then the other models are useful. If the other models are not better than the baseline model, then the other models are not useful.\n# Baseline Model\nbaseline_model = train['saleprice'].mean() # 181469.70160897123\n# The baseline model is the mean of the target variable. The baseline model is used to determine if the other models are better than the baseline model. If the other models are better than the baseline model, then the other models are useful. If the other models are not better than the baseline model, then the other models are not useful.\n# %% [markdown]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "original_test",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "original_test = [str(col).replace(' ','_').lower() for col in original_test.columns]\n# %%\n# Generate the Baseline Model\n# The baseline model is the model that is used to compare the performance of the other models. It is the simplest model that can be used to predict the target variable. In this case, the baseline model is the mean of the target variable.\n# The baseline model is used to determine if the other models are better than the baseline model. If the other models are better than the baseline model, then the other models are useful. If the other models are not better than the baseline model, then the other models are not useful.\n# Baseline Model\nbaseline_model = train['saleprice'].mean() # 181469.70160897123\n# The baseline model is the mean of the target variable. The baseline model is used to determine if the other models are better than the baseline model. If the other models are better than the baseline model, then the other models are useful. If the other models are not better than the baseline model, then the other models are not useful.\n# %% [markdown]\n#",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "baseline_model",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "baseline_model = train['saleprice'].mean() # 181469.70160897123\n# The baseline model is the mean of the target variable. The baseline model is used to determine if the other models are better than the baseline model. If the other models are better than the baseline model, then the other models are useful. If the other models are not better than the baseline model, then the other models are not useful.\n# %% [markdown]\n#\n# %% [markdown]\n# # EDA\n# %%\ntrain.shape, test.shape\n# %%\n# Drop all columns with more than 50% missing values (NaN)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train = train.dropna(thresh=len(train)*0.5, axis=1)\n#? Note: should I be dropping this from the test set?\ntest = test.dropna(thresh=len(test)*0.5, axis=1)\n# %%\ntrain.shape, test.shape\n# %%\ntrain.head()\n# %%\ntrain.info()\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test = test.dropna(thresh=len(test)*0.5, axis=1)\n# %%\ntrain.shape, test.shape\n# %%\ntrain.head()\n# %%\ntrain.info()\n# %%\ntrain.describe()\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 2\nfigure = plt.figure(figsize=(12,6))\nsns.scatterplot(x='lot_frontage', y='gr_liv_area', data=train)\nplt.title(f'Figure no. Lot Frontage vs Gr Liv Area')\nplt.savefig(f'../images/figure_no_{figure_number}_lot_frontage_vs_gr_liv_area.png')\nplt.show();\n# %% [markdown]\n# Lot frontage has an outlier beyond 300 and just under 200.  We will need to deal with these. The histogram shows that we can remove the outliers by capping the lot frontage at 125; however this would result in a loss of 410 rows versus capping at 150 which would result in a loss of 20 rows.  We will cap the lot frontage at 150. Now we have 1660 rows remaining in the dataset.\n#\n# ```python",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure = plt.figure(figsize=(12,6))\nsns.scatterplot(x='lot_frontage', y='gr_liv_area', data=train)\nplt.title(f'Figure no. Lot Frontage vs Gr Liv Area')\nplt.savefig(f'../images/figure_no_{figure_number}_lot_frontage_vs_gr_liv_area.png')\nplt.show();\n# %% [markdown]\n# Lot frontage has an outlier beyond 300 and just under 200.  We will need to deal with these. The histogram shows that we can remove the outliers by capping the lot frontage at 125; however this would result in a loss of 410 rows versus capping at 150 which would result in a loss of 20 rows.  We will cap the lot frontage at 150. Now we have 1660 rows remaining in the dataset.\n#\n# ```python\n# # I want to remove the outliers from the data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "trainee",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "trainee = train[train['lot_frontage'] < 300]\n# trainee.shape\n# so dropping the outliers didn't help much\ntrain.shape[0] - trainee.shape[0]\n# what percent of len(train) is 332?\n332/len(train)\n# %% [markdown]\n# So, if we windsorized our data we would lose 332 houses. which translates to roughly a 16% loss of our our total data.\n# %%\n# I want to remove the outliers from the data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "orig_rows",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "orig_rows = train.shape[0]\ntrain_tester = train[train['lot_frontage'] < 300]\ntrain_tester2 = train[train['gr_liv_area'] < 4000]\ntrain_tester.head()\nprint(train_tester.shape)\nprint(train_tester2.shape)\ntrain_tester = train_tester[train_tester['gr_liv_area'] < 4000]\nprint('removing outliers gr_liv_area < 4000')\nprint(train_tester.shape)\nprint(f'rows removed from train: {orig_rows - train_tester.shape[0]}')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train_tester",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train_tester = train[train['lot_frontage'] < 300]\ntrain_tester2 = train[train['gr_liv_area'] < 4000]\ntrain_tester.head()\nprint(train_tester.shape)\nprint(train_tester2.shape)\ntrain_tester = train_tester[train_tester['gr_liv_area'] < 4000]\nprint('removing outliers gr_liv_area < 4000')\nprint(train_tester.shape)\nprint(f'rows removed from train: {orig_rows - train_tester.shape[0]}')\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train_tester2",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train_tester2 = train[train['gr_liv_area'] < 4000]\ntrain_tester.head()\nprint(train_tester.shape)\nprint(train_tester2.shape)\ntrain_tester = train_tester[train_tester['gr_liv_area'] < 4000]\nprint('removing outliers gr_liv_area < 4000')\nprint(train_tester.shape)\nprint(f'rows removed from train: {orig_rows - train_tester.shape[0]}')\n# %%\n# what does the distribution of saleprice look like if these outliers are removed?",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train_tester",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train_tester = train_tester[train_tester['gr_liv_area'] < 4000]\nprint('removing outliers gr_liv_area < 4000')\nprint(train_tester.shape)\nprint(f'rows removed from train: {orig_rows - train_tester.shape[0]}')\n# %%\n# what does the distribution of saleprice look like if these outliers are removed?\nfigure_number = 3\nfigure = plt.figure(figsize=(12,6))\nsns.distplot(train_tester['saleprice'])\nplt.title('SalePrice Distribution without outliers in lot_frontage and gr_liv_area')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 3\nfigure = plt.figure(figsize=(12,6))\nsns.distplot(train_tester['saleprice'])\nplt.title('SalePrice Distribution without outliers in lot_frontage and gr_liv_area')\nplt.savefig(f'../images/figure_no_{figure_number}_saleprice_minuslotfrontage_grlivarea_outliers_distribution.png')\nplt.show();\n# %%\ntrain['gr_liv_area'].describe()\n# %%\n# I want to remove the outliers from the data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure = plt.figure(figsize=(12,6))\nsns.distplot(train_tester['saleprice'])\nplt.title('SalePrice Distribution without outliers in lot_frontage and gr_liv_area')\nplt.savefig(f'../images/figure_no_{figure_number}_saleprice_minuslotfrontage_grlivarea_outliers_distribution.png')\nplt.show();\n# %%\ntrain['gr_liv_area'].describe()\n# %%\n# I want to remove the outliers from the data\ntrain = train[train['lot_frontage'] < 300] # 332 rows removed",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train = train[train['lot_frontage'] < 300] # 332 rows removed\ntrain = train[train['gr_liv_area'] < 4000] # 1458 rows removed\n# %% [markdown]\n# I am interested in the impact that basements have on the saleprice, and to further examine this I want to engineer several features that relate to a house's basement.\n#\n# We are given the feature `bsmt_exposure` in our data which is a measure of how exposed the basement is. It is a categorical feature with the following values:\n# 1. GD - Good Exposure\n# 2. AV - Average Exposure (split levels or foyers typically score average or above)\n# 3. MN - Mimimum Exposure\n# 4. No - No Exposure",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train = train[train['gr_liv_area'] < 4000] # 1458 rows removed\n# %% [markdown]\n# I am interested in the impact that basements have on the saleprice, and to further examine this I want to engineer several features that relate to a house's basement.\n#\n# We are given the feature `bsmt_exposure` in our data which is a measure of how exposed the basement is. It is a categorical feature with the following values:\n# 1. GD - Good Exposure\n# 2. AV - Average Exposure (split levels or foyers typically score average or above)\n# 3. MN - Mimimum Exposure\n# 4. No - No Exposure\n#",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train['bsmt_exposure']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train['bsmt_exposure'] = train['bsmt_exposure'].map({'No': 0, 'Mn': 0.5, 'Av': 1, 'Gd': 2})\ntest['bsmt_exposure'] = test['bsmt_exposure'].map({'No': 0, 'Mn': 0.5, 'Av': 1, 'Gd': 2}) #& to the test set as well, so columns match.\n# %%\n# I want to create a feature that is the impact of the basement on the saleprice\n# I will do this by multiplying the basement exposure by the basement square footage\ntry:\n    train['bsmt_impact'] = train['bsmt_exposure'] * train['total_bsmt_sf']\n    test['bsmt_impact'] = test['bsmt_exposure'] * test['total_bsmt_sf']\nexcept Exception:\n    print(\"Please Run the notebook from the top to the bottom\")",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test['bsmt_exposure']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test['bsmt_exposure'] = test['bsmt_exposure'].map({'No': 0, 'Mn': 0.5, 'Av': 1, 'Gd': 2}) #& to the test set as well, so columns match.\n# %%\n# I want to create a feature that is the impact of the basement on the saleprice\n# I will do this by multiplying the basement exposure by the basement square footage\ntry:\n    train['bsmt_impact'] = train['bsmt_exposure'] * train['total_bsmt_sf']\n    test['bsmt_impact'] = test['bsmt_exposure'] * test['total_bsmt_sf']\nexcept Exception:\n    print(\"Please Run the notebook from the top to the bottom\")\n# %% [markdown]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train['bsmt_qual']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train['bsmt_qual'] = train['bsmt_qual'].replace({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0})\ntest['bsmt_qual'] = test['bsmt_qual'].replace({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}) #& test set too\n# %%\n# I want to create a feature that is the impact of the basement on the saleprice\ntrain['basement_volume'] = train['bsmt_qual'] * train['total_bsmt_sf']\ntest['basement_volume'] = test['bsmt_qual'] * test['total_bsmt_sf'] #& I need to do this for the test set as well\n# %%\n# I want to create a feature that is the impact of the basement on the saleprice\ntrain['basement_visible_surface_area'] = train['basement_volume'] * 0.25 # just a rough estimate\ntest['basement_visible_surface_area'] = test['basement_volume'] * 0.25 #& I need to do this for the test set as well",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test['bsmt_qual']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test['bsmt_qual'] = test['bsmt_qual'].replace({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}) #& test set too\n# %%\n# I want to create a feature that is the impact of the basement on the saleprice\ntrain['basement_volume'] = train['bsmt_qual'] * train['total_bsmt_sf']\ntest['basement_volume'] = test['bsmt_qual'] * test['total_bsmt_sf'] #& I need to do this for the test set as well\n# %%\n# I want to create a feature that is the impact of the basement on the saleprice\ntrain['basement_visible_surface_area'] = train['basement_volume'] * 0.25 # just a rough estimate\ntest['basement_visible_surface_area'] = test['basement_volume'] * 0.25 #& I need to do this for the test set as well\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train['basement_volume']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train['basement_volume'] = train['bsmt_qual'] * train['total_bsmt_sf']\ntest['basement_volume'] = test['bsmt_qual'] * test['total_bsmt_sf'] #& I need to do this for the test set as well\n# %%\n# I want to create a feature that is the impact of the basement on the saleprice\ntrain['basement_visible_surface_area'] = train['basement_volume'] * 0.25 # just a rough estimate\ntest['basement_visible_surface_area'] = test['basement_volume'] * 0.25 #& I need to do this for the test set as well\n# %%\ntrain['structure_square_footage'] = train['total_bsmt_sf'] + train['1st_flr_sf'] + train['2nd_flr_sf']\ntest['structure_square_footage'] = test['total_bsmt_sf'] + test['1st_flr_sf'] + test['2nd_flr_sf'] #& I need to do this for the test set as well\ntrain['structure_square_footage'].describe()",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test['basement_volume']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test['basement_volume'] = test['bsmt_qual'] * test['total_bsmt_sf'] #& I need to do this for the test set as well\n# %%\n# I want to create a feature that is the impact of the basement on the saleprice\ntrain['basement_visible_surface_area'] = train['basement_volume'] * 0.25 # just a rough estimate\ntest['basement_visible_surface_area'] = test['basement_volume'] * 0.25 #& I need to do this for the test set as well\n# %%\ntrain['structure_square_footage'] = train['total_bsmt_sf'] + train['1st_flr_sf'] + train['2nd_flr_sf']\ntest['structure_square_footage'] = test['total_bsmt_sf'] + test['1st_flr_sf'] + test['2nd_flr_sf'] #& I need to do this for the test set as well\ntrain['structure_square_footage'].describe()\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train['basement_visible_surface_area']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train['basement_visible_surface_area'] = train['basement_volume'] * 0.25 # just a rough estimate\ntest['basement_visible_surface_area'] = test['basement_volume'] * 0.25 #& I need to do this for the test set as well\n# %%\ntrain['structure_square_footage'] = train['total_bsmt_sf'] + train['1st_flr_sf'] + train['2nd_flr_sf']\ntest['structure_square_footage'] = test['total_bsmt_sf'] + test['1st_flr_sf'] + test['2nd_flr_sf'] #& I need to do this for the test set as well\ntrain['structure_square_footage'].describe()\n# %%\ntrain.columns\n# %%\ntest.columns",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test['basement_visible_surface_area']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test['basement_visible_surface_area'] = test['basement_volume'] * 0.25 #& I need to do this for the test set as well\n# %%\ntrain['structure_square_footage'] = train['total_bsmt_sf'] + train['1st_flr_sf'] + train['2nd_flr_sf']\ntest['structure_square_footage'] = test['total_bsmt_sf'] + test['1st_flr_sf'] + test['2nd_flr_sf'] #& I need to do this for the test set as well\ntrain['structure_square_footage'].describe()\n# %%\ntrain.columns\n# %%\ntest.columns\n# %% [markdown]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train['structure_square_footage']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train['structure_square_footage'] = train['total_bsmt_sf'] + train['1st_flr_sf'] + train['2nd_flr_sf']\ntest['structure_square_footage'] = test['total_bsmt_sf'] + test['1st_flr_sf'] + test['2nd_flr_sf'] #& I need to do this for the test set as well\ntrain['structure_square_footage'].describe()\n# %%\ntrain.columns\n# %%\ntest.columns\n# %% [markdown]\n# Something is wrong with a house with 334 square feet. I am going to check the price on the house and see if it is a typo.\n#",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test['structure_square_footage']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test['structure_square_footage'] = test['total_bsmt_sf'] + test['1st_flr_sf'] + test['2nd_flr_sf'] #& I need to do this for the test set as well\ntrain['structure_square_footage'].describe()\n# %%\ntrain.columns\n# %%\ntest.columns\n# %% [markdown]\n# Something is wrong with a house with 334 square feet. I am going to check the price on the house and see if it is a typo.\n#\n# ```python",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train = train[train['structure_square_footage'] != 334] # 1 row removed\n# %%\n# checking the distribution of structure_square_footage\nfigure_number = 4\nfigure = plt.figure(figsize=(12,6))\nsns.distplot(train['structure_square_footage'])\nplt.title(f'Figure no. {figure_number} Structure Square Footage Distribution')\nplt.savefig(f'../images/figure_no_{figure_number}_structure_square_footage_distribution.png')\nplt.show();\n# %% [markdown]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 4\nfigure = plt.figure(figsize=(12,6))\nsns.distplot(train['structure_square_footage'])\nplt.title(f'Figure no. {figure_number} Structure Square Footage Distribution')\nplt.savefig(f'../images/figure_no_{figure_number}_structure_square_footage_distribution.png')\nplt.show();\n# %% [markdown]\n# We also want to engineer the total square footage in the structure. This is the sum of the basement square footage and the first floor and second floor square footage. We will call this feature `structure_square_footage`.\n#\n# ```python",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure = plt.figure(figsize=(12,6))\nsns.distplot(train['structure_square_footage'])\nplt.title(f'Figure no. {figure_number} Structure Square Footage Distribution')\nplt.savefig(f'../images/figure_no_{figure_number}_structure_square_footage_distribution.png')\nplt.show();\n# %% [markdown]\n# We also want to engineer the total square footage in the structure. This is the sum of the basement square footage and the first floor and second floor square footage. We will call this feature `structure_square_footage`.\n#\n# ```python\n# # Creating a feature that is the total square footage of the structure",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "basement_df",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "basement_df = train[['bsmt_exposure', 'total_bsmt_sf', 'bsmt_impact','bsmt_qual','bsmt_cond','bsmtfin_type_1','bsmtfin_type_2']]\nbasement_df.head()\n# %% [markdown]\n# To eliminate any potential multicollinearity, I am going to drop the following features that were used to generate the new ones above.\n# * `bsmt_exposure`\n# * `bsmtfin_sf_1`\n# * `bsmtfin_sf_2`\n# * `1st_flr_sf`\n# * `2nd_flr_sf`\n# * `bsmt_qual`",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "used_cols",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "used_cols = ['bsmt_exposure', 'bsmt_qual', 'total_bsmt_sf','bsmtfin_sf_1', 'bsmtfin_sf_2','1st_flr_sf', '2nd_flr_sf']\ntrain.drop(columns = ['bsmt_exposure', 'bsmt_qual', 'total_bsmt_sf','bsmtfin_sf_1', 'bsmtfin_sf_2','1st_flr_sf', '2nd_flr_sf'], inplace=True)\ntest.drop(columns = ['bsmt_exposure', 'bsmt_qual', 'total_bsmt_sf','bsmtfin_sf_1', 'bsmtfin_sf_2','1st_flr_sf', '2nd_flr_sf'], inplace=True)\nprint(f'Removed columns from train: {used_cols}')\nprint(f'Removed columns from test: {used_cols}') #& I need to do this for the test set as well so the columns match at the end.\n# %%\ntrain['bsmt_impact'].value_counts()\n# %%\ntrain.head()\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train.drop(columns",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train.drop(columns = ['bsmt_exposure', 'bsmt_qual', 'total_bsmt_sf','bsmtfin_sf_1', 'bsmtfin_sf_2','1st_flr_sf', '2nd_flr_sf'], inplace=True)\ntest.drop(columns = ['bsmt_exposure', 'bsmt_qual', 'total_bsmt_sf','bsmtfin_sf_1', 'bsmtfin_sf_2','1st_flr_sf', '2nd_flr_sf'], inplace=True)\nprint(f'Removed columns from train: {used_cols}')\nprint(f'Removed columns from test: {used_cols}') #& I need to do this for the test set as well so the columns match at the end.\n# %%\ntrain['bsmt_impact'].value_counts()\n# %%\ntrain.head()\n# %%\n# I also would like to check the relationship between lot_frontage and gr_liv_area",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test.drop(columns",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test.drop(columns = ['bsmt_exposure', 'bsmt_qual', 'total_bsmt_sf','bsmtfin_sf_1', 'bsmtfin_sf_2','1st_flr_sf', '2nd_flr_sf'], inplace=True)\nprint(f'Removed columns from train: {used_cols}')\nprint(f'Removed columns from test: {used_cols}') #& I need to do this for the test set as well so the columns match at the end.\n# %%\ntrain['bsmt_impact'].value_counts()\n# %%\ntrain.head()\n# %%\n# I also would like to check the relationship between lot_frontage and gr_liv_area\nfigure_number = 5",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 5\nfigure = plt.figure(figsize=(12,6))\nsns.scatterplot(x='lot_frontage', y='gr_liv_area', data=train)\nplt.title(f'Figure no. {figure_number}. Lot Frontage vs Gr Liv Area')\nplt.savefig(f'../images/figure_no_{figure_number}_lot_frontage_vs_gr_liv_area.png')\nplt.show()\n# %%\n# show the distribution of lot_frontage\nfigure_number = 6\nfigure = plt.figure(figsize=(12,6))",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure = plt.figure(figsize=(12,6))\nsns.scatterplot(x='lot_frontage', y='gr_liv_area', data=train)\nplt.title(f'Figure no. {figure_number}. Lot Frontage vs Gr Liv Area')\nplt.savefig(f'../images/figure_no_{figure_number}_lot_frontage_vs_gr_liv_area.png')\nplt.show()\n# %%\n# show the distribution of lot_frontage\nfigure_number = 6\nfigure = plt.figure(figsize=(12,6))\nsns.distplot(train['lot_frontage'])",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 6\nfigure = plt.figure(figsize=(12,6))\nsns.distplot(train['lot_frontage'])\nplt.title(f'Figure no. {figure_number}. Lot Frontage Distribution')\nplt.savefig(f'../images/figure_no_{figure_number}_lot_frontage_distribution.png')\nplt.show();\n# %% [markdown]\n# ## Some Notes on the Data\n# %% [markdown]\n# 1. Pool Area has an average of 2.30 which really doesn't mean very much. It should be noted that the max for pool area is 800 and there are 1889 houses with pools in the Ames dataset.",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure = plt.figure(figsize=(12,6))\nsns.distplot(train['lot_frontage'])\nplt.title(f'Figure no. {figure_number}. Lot Frontage Distribution')\nplt.savefig(f'../images/figure_no_{figure_number}_lot_frontage_distribution.png')\nplt.show();\n# %% [markdown]\n# ## Some Notes on the Data\n# %% [markdown]\n# 1. Pool Area has an average of 2.30 which really doesn't mean very much. It should be noted that the max for pool area is 800 and there are 1889 houses with pools in the Ames dataset.\n# 2. Some houses have 0 for a feature which means that the house does not have that feature.  For example, if a house has 0 for the 3SsnPorch feature, it means that the house does not have a 3 season porch. Introducing a feature with 0 as a value will bias the model by adding complexity without adding any predictive power. We will need to be aware of this during analysis.",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure = plt.figure(figsize=(12,6))\nfigure_number = 7\nsns.distplot(train['lot_frontage'])\nplt.title(f'Figure no. {figure_number}. Lot Frontage Distribution')\nplt.savefig(f'../images/figure_no_{figure_number}_lot_frontage_distribution_hist.png')\nplt.show();\n# %%\ntrain[train['year_built'] <= 2010].head()\n# %% [markdown]\n# How many houses are there in the dataset that were built after 2006 and before 2008?",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 7\nsns.distplot(train['lot_frontage'])\nplt.title(f'Figure no. {figure_number}. Lot Frontage Distribution')\nplt.savefig(f'../images/figure_no_{figure_number}_lot_frontage_distribution_hist.png')\nplt.show();\n# %%\ntrain[train['year_built'] <= 2010].head()\n# %% [markdown]\n# How many houses are there in the dataset that were built after 2006 and before 2008?\n# ```python",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "thetweeners",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "thetweeners = train[(train['year_built'] > 2006) & (train['year_built'] < 2008)]\nthetweeners.describe()\n# %% [markdown]\n#\n# `mas_vnr_area` is the masonry veneer area in square feet. This means that the house has bricks or stone on the exterior walls. Some houses in the dataset have 0 square feet of mas_vnr_area.\n#\n# ```python\n# # just show these houses\n# train.loc[(train['mas_vnr_area'].isnull()) | (train['mas_vnr_area'] == 0)]\n# ```",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "ccf_features_basic",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "ccf_features_basic = ['lot_area', 'garage_type', 'garage_yr_blt',     'garage_finish','garage_area', 'garage_qual', 'garage_cond',\n       'kitchen_qual', 'wood_deck_sf', 'open_porch_sf', 'enclosed_porch',\n       '3ssn_porch', 'screen_porch', 'pool_area', 'fireplace_qu',\n       'gr_liv_area', 'low_qual_fin_sf', 'year_remod/add',\n       'exterior_1st', 'exterior_2nd', 'mas_vnr_area', 'mas_vnr_type',\n       'year_built', 'yr_sold','bsmt_impact',\n       'basement_volume', 'basement_visible_surface_area',\n       'structure_square_footage'] + ['saleprice']\n#^ I removed garage_cars because the garage is often closed. I want to see if the garage area is a better predictor of saleprice.\n#* garage_cars",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "#ccf_features_basic",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "#ccf_features_basic = ['1st_flr_sf','2nd_flr_sf','lot_area'] + ['saleprice']\n# %%\n# CCF Features - the features listed above.\ntrain = train[ccf_features_basic] # select only the features listed above\ntest = test[ccf_features_basic[:-1]] # I want to make sure the test set has the same columns as the train set.\n# %%\ntrain.head()\n# %% [markdown]\n# columns to dummify: exterior_1st, exterior_2nd, mas_vnr_type, garage_type, garage_finish, garage_qual, garage_cond, pool_qc, fence, fireplace_qu\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train = train[ccf_features_basic] # select only the features listed above\ntest = test[ccf_features_basic[:-1]] # I want to make sure the test set has the same columns as the train set.\n# %%\ntrain.head()\n# %% [markdown]\n# columns to dummify: exterior_1st, exterior_2nd, mas_vnr_type, garage_type, garage_finish, garage_qual, garage_cond, pool_qc, fence, fireplace_qu\n# %%\ntrain.drop(columns=['garage_yr_blt'], inplace=True)\n# %%\n#exterior_1st, exterior_2nd, mas_vnr_type, garage_type, garage_finish, garage_qual, garage_cond, pool_qc, fence, fireplace_qu",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test = test[ccf_features_basic[:-1]] # I want to make sure the test set has the same columns as the train set.\n# %%\ntrain.head()\n# %% [markdown]\n# columns to dummify: exterior_1st, exterior_2nd, mas_vnr_type, garage_type, garage_finish, garage_qual, garage_cond, pool_qc, fence, fireplace_qu\n# %%\ntrain.drop(columns=['garage_yr_blt'], inplace=True)\n# %%\n#exterior_1st, exterior_2nd, mas_vnr_type, garage_type, garage_finish, garage_qual, garage_cond, pool_qc, fence, fireplace_qu\ndummified_features_df = pd.get_dummies(train, columns=['garage_type', 'garage_finish', 'garage_qual', 'garage_cond', 'kitchen_qual', 'fireplace_qu', 'exterior_1st', 'exterior_2nd', 'mas_vnr_type'], drop_first=True)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "dummified_features_df",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "dummified_features_df = pd.get_dummies(train, columns=['garage_type', 'garage_finish', 'garage_qual', 'garage_cond', 'kitchen_qual', 'fireplace_qu', 'exterior_1st', 'exterior_2nd', 'mas_vnr_type'], drop_first=True)\n# %%\ntrain.info()\n# %% [markdown]\n# Not enough data on garage_type,garage_finish,or garage_cond, or garage_qual, as well as fireplace_qu\n#\n# %%\n# drop those:\ntrain = train.drop(columns=['garage_type', 'garage_finish', 'garage_qual', 'garage_cond', 'kitchen_qual', 'fireplace_qu'])\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train = train.drop(columns=['garage_type', 'garage_finish', 'garage_qual', 'garage_cond', 'kitchen_qual', 'fireplace_qu'])\n# %%\ntrain.head()\n# %%\nfor col in test.columns:\n    if col not in train.columns:\n        test.drop(columns=col, inplace=True)\n        print(f'Dropped {col} from test set.')\n        ccf_features_basic.remove(col) # also remove the column from the list of features\n# justification: When testing the model on the test set, I want to make sure the test set has the same columns as the train set.",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 8\n# center the plots on the figure\nax, fig = plt.subplots(figsize=(30,15))\n# only plot numeric features\nfor i, col in enumerate(train.select_dtypes('number').columns):\n    plt.subplot(5, 5, i+1)\n    sns.distplot(train[col])\n    plt.title(col);\nfigure.subplots_adjust(hspace=0.5, wspace=0.5)\nplt.tight_layout()",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "dummies",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "dummies = pd.get_dummies(train, columns = ['exterior_1st','exterior_2nd','mas_vnr_type'], drop_first=True)\ndummies_test = pd.get_dummies(test, columns = ['exterior_1st','exterior_2nd','mas_vnr_type'], drop_first=True)\ndummies.head() # is train + dummies\n# %%\n# move saleprice to the end again\nsaleprice = dummies['saleprice']\ndummies.drop(columns=['saleprice'], inplace=True)\ndummies['saleprice'] = saleprice\ndummies_test.to_csv('../data/test_dummies.csv', index=False)\ndummies.to_csv('../data/train_dummies.csv', index=False)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "dummies_test",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "dummies_test = pd.get_dummies(test, columns = ['exterior_1st','exterior_2nd','mas_vnr_type'], drop_first=True)\ndummies.head() # is train + dummies\n# %%\n# move saleprice to the end again\nsaleprice = dummies['saleprice']\ndummies.drop(columns=['saleprice'], inplace=True)\ndummies['saleprice'] = saleprice\ndummies_test.to_csv('../data/test_dummies.csv', index=False)\ndummies.to_csv('../data/train_dummies.csv', index=False)\ntest = pd.read_csv('../data/test_dummies.csv') #& I need to do this for the test set as well",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "saleprice",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "saleprice = dummies['saleprice']\ndummies.drop(columns=['saleprice'], inplace=True)\ndummies['saleprice'] = saleprice\ndummies_test.to_csv('../data/test_dummies.csv', index=False)\ndummies.to_csv('../data/train_dummies.csv', index=False)\ntest = pd.read_csv('../data/test_dummies.csv') #& I need to do this for the test set as well\n# train = pd.read_csv('../data/train_dummies.csv')\ntrain = dummies\ndummies.head()\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "dummies['saleprice']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "dummies['saleprice'] = saleprice\ndummies_test.to_csv('../data/test_dummies.csv', index=False)\ndummies.to_csv('../data/train_dummies.csv', index=False)\ntest = pd.read_csv('../data/test_dummies.csv') #& I need to do this for the test set as well\n# train = pd.read_csv('../data/train_dummies.csv')\ntrain = dummies\ndummies.head()\n# %%\n# remove 'exterior_1st','exterior_2nd' and 'mas_vnr_type', from ccf_features_basic\nccf_features_basic.remove('exterior_1st')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test = pd.read_csv('../data/test_dummies.csv') #& I need to do this for the test set as well\n# train = pd.read_csv('../data/train_dummies.csv')\ntrain = dummies\ndummies.head()\n# %%\n# remove 'exterior_1st','exterior_2nd' and 'mas_vnr_type', from ccf_features_basic\nccf_features_basic.remove('exterior_1st')\nccf_features_basic.remove('exterior_2nd')\nccf_features_basic.remove('mas_vnr_type')\n# Reason: I have dummified these columns, so I don't need them anymore.",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train = dummies\ndummies.head()\n# %%\n# remove 'exterior_1st','exterior_2nd' and 'mas_vnr_type', from ccf_features_basic\nccf_features_basic.remove('exterior_1st')\nccf_features_basic.remove('exterior_2nd')\nccf_features_basic.remove('mas_vnr_type')\n# Reason: I have dummified these columns, so I don't need them anymore.\n# %%\ntrain.head()",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "corr_matrix",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "corr_matrix = train[ccf_features_basic].corr()\ncorr_matrix['saleprice'].sort_values(ascending=False)\n# I will use the correlation matrix to determine which features are highly correlated with each other.\n# I will use a heatmap to visualize it with a mask over the diagonal.\nfigure_number  = 9\nmask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix, mask = mask, annot=True, cmap='coolwarm')\n# save the plot",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "mask",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "mask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix, mask = mask, annot=True, cmap='coolwarm')\n# save the plot\nplt.title(f'Figure {figure_number}: CCF Correlation Matrix', fontsize=20)\nplt.savefig(f'../images/figure_no_{figure_number}_heatmap.png')\nplt.show();\n# %% [markdown]\n# I need to determine which features are highly correlated with each other. I will use a heatmap to visualize it with a mask over the diagonal. This is to eliminate multicollinearity.",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "mask[np.triu_indices_from(mask)]",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "mask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix, mask = mask, annot=True, cmap='coolwarm')\n# save the plot\nplt.title(f'Figure {figure_number}: CCF Correlation Matrix', fontsize=20)\nplt.savefig(f'../images/figure_no_{figure_number}_heatmap.png')\nplt.show();\n# %% [markdown]\n# I need to determine which features are highly correlated with each other. I will use a heatmap to visualize it with a mask over the diagonal. This is to eliminate multicollinearity.\n#",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "vif",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "vif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(train[ccf_features_basic].values, i) for i in range(train[ccf_features_basic].shape[1])]\nvif[\"features\"] = train[ccf_features_basic].columns\nvif.head(100)\n# %% [markdown]\n# Any features that have a vif over five will be dropped.\n#\n# %%\n# only include features with a VIF of less than 5 and not equal to 'inf' or np.Inf\n# filter vif table to only include features with a VIF of less than 5, that is not salesprice or one of my engineered features and not equal to 'inf' or np.Inf",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "vif[\"features\"]",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "vif[\"features\"] = train[ccf_features_basic].columns\nvif.head(100)\n# %% [markdown]\n# Any features that have a vif over five will be dropped.\n#\n# %%\n# only include features with a VIF of less than 5 and not equal to 'inf' or np.Inf\n# filter vif table to only include features with a VIF of less than 5, that is not salesprice or one of my engineered features and not equal to 'inf' or np.Inf\n# vif = vif[((vif['VIF Factor'] < 5) & (vif['VIF Factor'] != np.inf) & (vif['VIF Factor'] != 'inf') & (vif['features'] != 'saleprice')) & ((vif['features'] == 'basement_volume') | (vif['features'] == 'basement_visible_surface_area') | (vif['features'] == 'structure_square_footage'))]\n#vif = vif[(vif['VIF Factor'] < 5) & (vif['VIF Factor'] != np.inf) & (vif['VIF Factor'] != 'inf') & (vif['features'] != 'saleprice')]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "#vif",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "#vif = vif[(vif['VIF Factor'] < 5) & (vif['VIF Factor'] != np.inf) & (vif['VIF Factor'] != 'inf') & (vif['features'] != 'saleprice')]\nvif_to_drop = vif[(vif['VIF Factor'] > 5) & (vif['VIF Factor'] != np.inf) & (vif['VIF Factor'] != 'inf') & (vif['features'] != 'saleprice')]\nvif_to_drop.head(15)\n# %%\n# remove features with a VIF of more than 5 from train and ccf_features_basic\nprint(f'train.shape before removing np.Inf: {train.shape}')\ntrain.drop(columns=vif_to_drop['features'], inplace=True)\nfor feature in vif_to_drop['features']:\n    ccf_features_basic.remove(feature)\nprint(f'train.shape before removing np.Inf: {train.shape}')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "vif_to_drop",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "vif_to_drop = vif[(vif['VIF Factor'] > 5) & (vif['VIF Factor'] != np.inf) & (vif['VIF Factor'] != 'inf') & (vif['features'] != 'saleprice')]\nvif_to_drop.head(15)\n# %%\n# remove features with a VIF of more than 5 from train and ccf_features_basic\nprint(f'train.shape before removing np.Inf: {train.shape}')\ntrain.drop(columns=vif_to_drop['features'], inplace=True)\nfor feature in vif_to_drop['features']:\n    ccf_features_basic.remove(feature)\nprint(f'train.shape before removing np.Inf: {train.shape}')\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train = train.replace(np.inf, np.nan)\nprint(f'train.shape: {train.shape}')\n# %%\ntrain.dtypes\n# %%\nfigure_number = 10\nplt.figure(figsize=(25,30))\n# make barplot of all features and their correlation with saleprice except for the target\nsns.barplot(x=train.corr()['saleprice'].sort_values(ascending=False)[1:], y=train.corr()['saleprice'].sort_values(ascending=False)[1:].index)\nplt.yticks(fontsize=20)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 10\nplt.figure(figsize=(25,30))\n# make barplot of all features and their correlation with saleprice except for the target\nsns.barplot(x=train.corr()['saleprice'].sort_values(ascending=False)[1:], y=train.corr()['saleprice'].sort_values(ascending=False)[1:].index)\nplt.yticks(fontsize=20)\nplt.title(f'Figure {figure_number}: Correlation of CCF Features with Saleprice', fontsize=20)\nplt.savefig(f'../images/figure_no_{figure_number}_barplot.png')\nplt.show();\n# %% [markdown]\n# Creating a figure like the one above to illustrate the coefficients for our linear regression model.",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "corr_matrix",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "corr_matrix = train[ccf_features_basic].corr()\ncorr_matrix['saleprice'].sort_values(ascending=False)\n# I will use the correlation matrix to determine which features are highly correlated with each other.\n# I will use a heatmap to visualize it with a mask over the diagonal.\nfigure_number = 11\nmask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix, mask = mask, annot=True, cmap='coolwarm',)\n# save the plot",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 11\nmask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix, mask = mask, annot=True, cmap='coolwarm',)\n# save the plot\nplt.xticks(fontsize=20)\nplt.tight_layout()\nplt.title(f'Figure {figure_number}: CCF Correlation Matrix', fontsize=20)\nplt.savefig(f'../images/figure_no_{figure_number}_heatmap.png')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "mask",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "mask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix, mask = mask, annot=True, cmap='coolwarm',)\n# save the plot\nplt.xticks(fontsize=20)\nplt.tight_layout()\nplt.title(f'Figure {figure_number}: CCF Correlation Matrix', fontsize=20)\nplt.savefig(f'../images/figure_no_{figure_number}_heatmap.png')\nplt.show();",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "mask[np.triu_indices_from(mask)]",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "mask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix, mask = mask, annot=True, cmap='coolwarm',)\n# save the plot\nplt.xticks(fontsize=20)\nplt.tight_layout()\nplt.title(f'Figure {figure_number}: CCF Correlation Matrix', fontsize=20)\nplt.savefig(f'../images/figure_no_{figure_number}_heatmap.png')\nplt.show();\n# %% [markdown]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "corr_matrix",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "corr_matrix = train[ccf_features_basic].corr()\ncorr_matrix['saleprice'].sort_values(ascending=False)\n# I will use the correlation matrix to determine which features are highly correlated with each other.\n# I will use a heatmap to visualize it with a mask over the diagonal.\nfigure_number = 12\nmask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix, mask = mask, annot=True, cmap='coolwarm')\n# save the plot",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 12\nmask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix, mask = mask, annot=True, cmap='coolwarm')\n# save the plot\nplt.title(f'Figure {figure_number}: CCF Correlation Matrix', fontsize=30, y=0.97)\nplt.yticks(fontsize=20)\n# add some padding to the top of the plot\nplt.savefig(f'../images/figure_no_{figure_number}_heatmap.png')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "mask",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "mask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix, mask = mask, annot=True, cmap='coolwarm')\n# save the plot\nplt.title(f'Figure {figure_number}: CCF Correlation Matrix', fontsize=30, y=0.97)\nplt.yticks(fontsize=20)\n# add some padding to the top of the plot\nplt.savefig(f'../images/figure_no_{figure_number}_heatmap.png')\nplt.show();",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "mask[np.triu_indices_from(mask)]",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "mask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_matrix, mask = mask, annot=True, cmap='coolwarm')\n# save the plot\nplt.title(f'Figure {figure_number}: CCF Correlation Matrix', fontsize=30, y=0.97)\nplt.yticks(fontsize=20)\n# add some padding to the top of the plot\nplt.savefig(f'../images/figure_no_{figure_number}_heatmap.png')\nplt.show();\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "ccf_features",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "ccf_features = corr_matrix[corr_matrix['saleprice'] > 0.25].index.tolist()\nprint(ccf_features)\n# %% [markdown]\n# Let's look at just the `saleprice` column as a heatmap to see the correlation between the features and the target variable.\n#\n#\n# ```python\n# # create a heatmap column of all features and their correlation to saleprice\n# plt.figure(figsize=(12, 12))\n# sns.heatmap(train[ccf_features_basic].corr()[['saleprice']].sort_values(by='saleprice', ascending=False), annot=True, cmap='coolwarm')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 13\nplt.figure(figsize=(12, 12))\nsns.heatmap(train[ccf_features_basic].corr()[['saleprice']].sort_values(by='saleprice', ascending=False)[1:], annot=True, cmap='coolwarm')\nplt.title(f'Figure {figure_number}: Correlation of CCF Features with Saleprice', fontsize=20)\nplt.tight_layout()\nplt.savefig(f'../images/figure_no_{figure_number}_singlecolumn_heatmap.png')\nplt.show()\n# %% [markdown]\n#\n# %% [markdown]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_scores",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_scores = pd.DataFrame() # to hold all model scores\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\nrow = {} # to hold a single row of model_scores\n# %%\ntrain = train[ccf_features]\n# %%\n# the baseline model\n# I will use the mean of the saleprice as the baseline model\nb = train['saleprice'].mean()\nmodel_predictions = {} # dictionary to store the predictions of the models",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row = {} # to hold a single row of model_scores\n# %%\ntrain = train[ccf_features]\n# %%\n# the baseline model\n# I will use the mean of the saleprice as the baseline model\nb = train['saleprice'].mean()\nmodel_predictions = {} # dictionary to store the predictions of the models\nmodel_scores = {} # dictionary to store the scores of the models\n# %% [markdown]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train = train[ccf_features]\n# %%\n# the baseline model\n# I will use the mean of the saleprice as the baseline model\nb = train['saleprice'].mean()\nmodel_predictions = {} # dictionary to store the predictions of the models\nmodel_scores = {} # dictionary to store the scores of the models\n# %% [markdown]\n# ## Linear Regression Model\n#",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "b",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "b = train['saleprice'].mean()\nmodel_predictions = {} # dictionary to store the predictions of the models\nmodel_scores = {} # dictionary to store the scores of the models\n# %% [markdown]\n# ## Linear Regression Model\n#\n# %%\n# Building our Model\nlr = LinearRegression()\nX = train.drop(columns=['saleprice'])",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_predictions",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_predictions = {} # dictionary to store the predictions of the models\nmodel_scores = {} # dictionary to store the scores of the models\n# %% [markdown]\n# ## Linear Regression Model\n#\n# %%\n# Building our Model\nlr = LinearRegression()\nX = train.drop(columns=['saleprice'])\ny = train['saleprice'] # target",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_scores",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_scores = {} # dictionary to store the scores of the models\n# %% [markdown]\n# ## Linear Regression Model\n#\n# %%\n# Building our Model\nlr = LinearRegression()\nX = train.drop(columns=['saleprice'])\ny = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42) # split the data into train and validation sets",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "lr = LinearRegression()\nX = train.drop(columns=['saleprice'])\ny = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42) # split the data into train and validation sets\nlr.fit(X_train,y_train) # fit on training data\nlr.score(X_train, y_train) # score on training data\n# Predictions and Evaluation of the Model\ny_val_preds = lr.predict(X_val) # predict on validation data\nprint(f'training score: {lr.score(X_train, y_train)}') # score on training data\nprint(f'validataion score: {lr.score(X_val, y_val)}') # score on validation data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "X = train.drop(columns=['saleprice'])\ny = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42) # split the data into train and validation sets\nlr.fit(X_train,y_train) # fit on training data\nlr.score(X_train, y_train) # score on training data\n# Predictions and Evaluation of the Model\ny_val_preds = lr.predict(X_val) # predict on validation data\nprint(f'training score: {lr.score(X_train, y_train)}') # score on training data\nprint(f'validataion score: {lr.score(X_val, y_val)}') # score on validation data\nprint(f'cross_val_score: {cross_val_score(lr, X_train, y_train, cv=5).mean()}') # cross_val_score on training data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "y = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42) # split the data into train and validation sets\nlr.fit(X_train,y_train) # fit on training data\nlr.score(X_train, y_train) # score on training data\n# Predictions and Evaluation of the Model\ny_val_preds = lr.predict(X_val) # predict on validation data\nprint(f'training score: {lr.score(X_train, y_train)}') # score on training data\nprint(f'validataion score: {lr.score(X_val, y_val)}') # score on validation data\nprint(f'cross_val_score: {cross_val_score(lr, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\nlr_score = lr.score(X_val, y_val) # score on validation data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "y_val_preds",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "y_val_preds = lr.predict(X_val) # predict on validation data\nprint(f'training score: {lr.score(X_train, y_train)}') # score on training data\nprint(f'validataion score: {lr.score(X_val, y_val)}') # score on validation data\nprint(f'cross_val_score: {cross_val_score(lr, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\nlr_score = lr.score(X_val, y_val) # score on validation data\nmodel_scores['lr'] = lr_score # add the score to the model_scores dictionary\nmodel_predictions['lr'] = y_val_preds # add the predictions to the model_predictions dictionary\ntrain_score = lr.score(X_train, y_train) # score on training data\ntest_score = lr.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(lr, X_train, y_train, cv=5).mean() # score on cross validation data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "lr_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "lr_score = lr.score(X_val, y_val) # score on validation data\nmodel_scores['lr'] = lr_score # add the score to the model_scores dictionary\nmodel_predictions['lr'] = y_val_preds # add the predictions to the model_predictions dictionary\ntrain_score = lr.score(X_train, y_train) # score on training data\ntest_score = lr.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(lr, X_train, y_train, cv=5).mean() # score on cross validation data\n# %% [markdown]\n#\n# %%\n# Plot the predictions vs the actual values",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_scores['lr']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_scores['lr'] = lr_score # add the score to the model_scores dictionary\nmodel_predictions['lr'] = y_val_preds # add the predictions to the model_predictions dictionary\ntrain_score = lr.score(X_train, y_train) # score on training data\ntest_score = lr.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(lr, X_train, y_train, cv=5).mean() # score on cross validation data\n# %% [markdown]\n#\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 14",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_predictions['lr']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_predictions['lr'] = y_val_preds # add the predictions to the model_predictions dictionary\ntrain_score = lr.score(X_train, y_train) # score on training data\ntest_score = lr.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(lr, X_train, y_train, cv=5).mean() # score on cross validation data\n# %% [markdown]\n#\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 14\nmodel = 'LinearRegression'",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train_score = lr.score(X_train, y_train) # score on training data\ntest_score = lr.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(lr, X_train, y_train, cv=5).mean() # score on cross validation data\n# %% [markdown]\n#\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 14\nmodel = 'LinearRegression'\nplt.figure(figsize=(10, 10))",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test_score = lr.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(lr, X_train, y_train, cv=5).mean() # score on cross validation data\n# %% [markdown]\n#\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 14\nmodel = 'LinearRegression'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "cval_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "cval_score = cross_val_score(lr, X_train, y_train, cv=5).mean() # score on cross validation data\n# %% [markdown]\n#\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 14\nmodel = 'LinearRegression'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 14\nmodel = 'LinearRegression'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\n# remove grid\nplt.grid(False)\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\nplt.annotate(f'model: {model} \\ntrain:{train_score}\\ntest:{test_score}\\ncval:{cval_score}', xy=(0.05, 0.75), xycoords='axes fraction')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model = 'LinearRegression'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\n# remove grid\nplt.grid(False)\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\nplt.annotate(f'model: {model} \\ntrain:{train_score}\\ntest:{test_score}\\ncval:{cval_score}', xy=(0.05, 0.75), xycoords='axes fraction')\nplt.title(f'Figure {figure_number}: {model} Predicted vs Actual Sale Price', fontsize=20)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 15\nmodel = 'LinearRegression'\nfigure = plt.figure(figsize=(10, 10))\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.annotate(f'model: {model} \\nR2: {lr_score}', xy=(0.1, 0.9), xycoords='axes fraction', fontsize=15)\nplt.title(f'Figure {figure_number}: LR Residuals', fontsize=20)\nplt.savefig(f'../images/figure_no_{figure_number}_{model}_residuals.png')\nplt.show();\n# %% [markdown]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model = 'LinearRegression'\nfigure = plt.figure(figsize=(10, 10))\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.annotate(f'model: {model} \\nR2: {lr_score}', xy=(0.1, 0.9), xycoords='axes fraction', fontsize=15)\nplt.title(f'Figure {figure_number}: LR Residuals', fontsize=20)\nplt.savefig(f'../images/figure_no_{figure_number}_{model}_residuals.png')\nplt.show();\n# %% [markdown]\n#",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure = plt.figure(figsize=(10, 10))\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.annotate(f'model: {model} \\nR2: {lr_score}', xy=(0.1, 0.9), xycoords='axes fraction', fontsize=15)\nplt.title(f'Figure {figure_number}: LR Residuals', fontsize=20)\nplt.savefig(f'../images/figure_no_{figure_number}_{model}_residuals.png')\nplt.show();\n# %% [markdown]\n#\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "lr_coefs",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "lr_coefs = pd.DataFrame(lr.coef_, X.columns, columns=[\"Coefficients\"])\n# %%\n# plot the coefficients\n# coefficients[coefficients[\"Coefficients\"] > 0].sort_values(by=\"Coefficients\").plot(kind=\"barh\")\n# spread out the y labels\nlr_coefs.head(200).sort_values(by=\"Coefficients\", ascending=False)\n# %%\n# Inference variables\ncoefficients = lr_coefs # set to linear regression\nlargest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[0]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "coefficients",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "coefficients = lr_coefs # set to linear regression\nlargest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[0]\nlargest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} increase in saleprice.')\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} increase in saleprice.')\nlargest_coefs_four_lr = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_lr = coefficients['Coefficients'].sort_values(ascending=False)[-4:]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "largest_coef_value",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "largest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[0]\nlargest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} increase in saleprice.')\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} increase in saleprice.')\nlargest_coefs_four_lr = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_lr = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "largest_coef_feature",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "largest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} increase in saleprice.')\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} increase in saleprice.')\nlargest_coefs_four_lr = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_lr = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\nmodel_scores_df = pd.DataFrame() # to hold all model scores",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "smallest_coef_value",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "smallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} increase in saleprice.')\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} increase in saleprice.')\nlargest_coefs_four_lr = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_lr = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\nmodel_scores_df = pd.DataFrame() # to hold all model scores\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "smallest_coef_feature",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "smallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} increase in saleprice.')\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} increase in saleprice.')\nlargest_coefs_four_lr = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_lr = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\nmodel_scores_df = pd.DataFrame() # to hold all model scores\n# %%\n# add the model scores to the model_scores dataframe",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "largest_coefs_four_lr",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "largest_coefs_four_lr = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_lr = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\nmodel_scores_df = pd.DataFrame() # to hold all model scores\n# %%\n# add the model scores to the model_scores dataframe\nrow['model_name'] = 'LinearRegression'\nrow['r2_score'] = lr_score\nrow['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "smallest_coefs_four_lr",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "smallest_coefs_four_lr = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\nmodel_scores_df = pd.DataFrame() # to hold all model scores\n# %%\n# add the model scores to the model_scores dataframe\nrow['model_name'] = 'LinearRegression'\nrow['r2_score'] = lr_score\nrow['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_scores_df",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_scores_df = pd.DataFrame() # to hold all model scores\n# %%\n# add the model scores to the model_scores dataframe\nrow['model_name'] = 'LinearRegression'\nrow['r2_score'] = lr_score\nrow['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['model_name']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['model_name'] = 'LinearRegression'\nrow['r2_score'] = lr_score\nrow['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['r2_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['r2_score'] = lr_score\nrow['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%\nmodel_scores_df.head()",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['rmse_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%\nmodel_scores_df.head()\n# %% [markdown]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['train_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%\nmodel_scores_df.head()\n# %% [markdown]\n# # Lasso Regression Model",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['test_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%\nmodel_scores_df.head()\n# %% [markdown]\n# # Lasso Regression Model\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['cval_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%\nmodel_scores_df.head()\n# %% [markdown]\n# # Lasso Regression Model\n# %%\n# Building our Lasso Model",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%\nmodel_scores_df.head()\n# %% [markdown]\n# # Lasso Regression Model\n# %%\n# Building our Lasso Model\n# import lassocv",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_scores_df",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_scores_df = model_scores_df.append(row, ignore_index=True)\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%\nmodel_scores_df.head()\n# %% [markdown]\n# # Lasso Regression Model\n# %%\n# Building our Lasso Model\n# import lassocv\n#! TODO - Scale before Lasso, .....",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "ss",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "ss = StandardScaler()\nfrom sklearn.linear_model import LassoCV\n# Init, fit, test Lasso Regressor\nalphas = np.logspace(-4, 0, 600) # create a list of alphas to test\nlasso = LassoCV(alphas=alphas, cv=5) # create a lasso regression model\nX = train.drop(columns=['saleprice'])\ny = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y,\n                                                  random_state=42) # split the data into train and validation sets\nX_train_lr = X_train",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "alphas",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "alphas = np.logspace(-4, 0, 600) # create a list of alphas to test\nlasso = LassoCV(alphas=alphas, cv=5) # create a lasso regression model\nX = train.drop(columns=['saleprice'])\ny = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y,\n                                                  random_state=42) # split the data into train and validation sets\nX_train_lr = X_train\nX_val_lr = X_val\nX_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "lasso",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "lasso = LassoCV(alphas=alphas, cv=5) # create a lasso regression model\nX = train.drop(columns=['saleprice'])\ny = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y,\n                                                  random_state=42) # split the data into train and validation sets\nX_train_lr = X_train\nX_val_lr = X_val\nX_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nlasso.fit(X_train,y_train) # fit on training data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "X = train.drop(columns=['saleprice'])\ny = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y,\n                                                  random_state=42) # split the data into train and validation sets\nX_train_lr = X_train\nX_val_lr = X_val\nX_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nlasso.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "y = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y,\n                                                  random_state=42) # split the data into train and validation sets\nX_train_lr = X_train\nX_val_lr = X_val\nX_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nlasso.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\n# y_preds = lasso.predict(X_train) # predict on training data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "X_train_lr",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "X_train_lr = X_train\nX_val_lr = X_val\nX_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nlasso.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\n# y_preds = lasso.predict(X_train) # predict on training data\ny_val_preds = lasso.predict(X_val) # predict on validation data\nprint(f'validataion score: {lasso.score(X_val, y_val)}') # score on validation data\n# print(mean_squared_error(y_val, y_preds)) # mean squared error on validation data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "X_val_lr",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "X_val_lr = X_val\nX_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nlasso.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\n# y_preds = lasso.predict(X_train) # predict on training data\ny_val_preds = lasso.predict(X_val) # predict on validation data\nprint(f'validataion score: {lasso.score(X_val, y_val)}') # score on validation data\n# print(mean_squared_error(y_val, y_preds)) # mean squared error on validation data\n# print(r2_score(y_val, y_preds)) # r2 score on validation data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "X_train",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "X_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nlasso.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\n# y_preds = lasso.predict(X_train) # predict on training data\ny_val_preds = lasso.predict(X_val) # predict on validation data\nprint(f'validataion score: {lasso.score(X_val, y_val)}') # score on validation data\n# print(mean_squared_error(y_val, y_preds)) # mean squared error on validation data\n# print(r2_score(y_val, y_preds)) # r2 score on validation data\nprint(f'training score: {lasso.score(X_train, y_train)}') # score on training data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "X_val",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "X_val = ss.transform(X_val)\nlasso.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\n# y_preds = lasso.predict(X_train) # predict on training data\ny_val_preds = lasso.predict(X_val) # predict on validation data\nprint(f'validataion score: {lasso.score(X_val, y_val)}') # score on validation data\n# print(mean_squared_error(y_val, y_preds)) # mean squared error on validation data\n# print(r2_score(y_val, y_preds)) # r2 score on validation data\nprint(f'training score: {lasso.score(X_train, y_train)}') # score on training data\ntrain_score = lasso.score(X_train, y_train) # score on training data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "y_val_preds",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "y_val_preds = lasso.predict(X_val) # predict on validation data\nprint(f'validataion score: {lasso.score(X_val, y_val)}') # score on validation data\n# print(mean_squared_error(y_val, y_preds)) # mean squared error on validation data\n# print(r2_score(y_val, y_preds)) # r2 score on validation data\nprint(f'training score: {lasso.score(X_train, y_train)}') # score on training data\ntrain_score = lasso.score(X_train, y_train) # score on training data\ntest_score = lasso.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(lasso, X_train, y_train, cv=5).mean() # score on cross validation data\n# add the score to the model_scores dictionary\nmodel_scores['lasso'] = lasso.score(X_val, y_val)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train_score = lasso.score(X_train, y_train) # score on training data\ntest_score = lasso.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(lasso, X_train, y_train, cv=5).mean() # score on cross validation data\n# add the score to the model_scores dictionary\nmodel_scores['lasso'] = lasso.score(X_val, y_val)\n# add the predictions to the model_predictions dictionary\nmodel_predictions['lasso'] = y_val_preds\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 15",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test_score = lasso.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(lasso, X_train, y_train, cv=5).mean() # score on cross validation data\n# add the score to the model_scores dictionary\nmodel_scores['lasso'] = lasso.score(X_val, y_val)\n# add the predictions to the model_predictions dictionary\nmodel_predictions['lasso'] = y_val_preds\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 15\nmodel_name = 'Lasso'",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "cval_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "cval_score = cross_val_score(lasso, X_train, y_train, cv=5).mean() # score on cross validation data\n# add the score to the model_scores dictionary\nmodel_scores['lasso'] = lasso.score(X_val, y_val)\n# add the predictions to the model_predictions dictionary\nmodel_predictions['lasso'] = y_val_preds\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 15\nmodel_name = 'Lasso'\nplt.figure(figsize=(10, 10))",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_scores['lasso']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_scores['lasso'] = lasso.score(X_val, y_val)\n# add the predictions to the model_predictions dictionary\nmodel_predictions['lasso'] = y_val_preds\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 15\nmodel_name = 'Lasso'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_predictions['lasso']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_predictions['lasso'] = y_val_preds\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 15\nmodel_name = 'Lasso'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 15\nmodel_name = 'Lasso'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\nplt.annotate(f'model: {model} \\ntrain:{train_score}\\ntest:{test_score}\\ncval:{cval_score}', xy=(0.05, 0.75), xycoords='axes fraction')\nplt.title(f'Figure {figure_number}: Lasso Predicted vs Actual Sale Price', fontsize=20)\nplt.savefig(f'../images/figure_no_{figure_number}_{model_name}_actual_vs_predicted.png')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_name = 'Lasso'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\nplt.annotate(f'model: {model} \\ntrain:{train_score}\\ntest:{test_score}\\ncval:{cval_score}', xy=(0.05, 0.75), xycoords='axes fraction')\nplt.title(f'Figure {figure_number}: Lasso Predicted vs Actual Sale Price', fontsize=20)\nplt.savefig(f'../images/figure_no_{figure_number}_{model_name}_actual_vs_predicted.png')\nlasso_score = lasso.score(X_val, y_val)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "lasso_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "lasso_score = lasso.score(X_val, y_val)\nplt.show()\n# %%\n# plot the residuals\nfigure_number = 16\nmodel_name = 'Lasso'\nfigure = plt.figure(figsize=(10, 10))\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.ylabel('Residuals')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 16\nmodel_name = 'Lasso'\nfigure = plt.figure(figsize=(10, 10))\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.ylabel('Residuals')\nplt.xlabel('Predicted Sale Price')\nplt.title(f'Figure {figure_number}: Lasso Residuals: {lasso.score(X_val,y_val)}', fontsize=20)\nplt.savefig(f'../images/figure_no_{figure_number}__{model_name}_residuals.png')\nplt.show();",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_name = 'Lasso'\nfigure = plt.figure(figsize=(10, 10))\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.ylabel('Residuals')\nplt.xlabel('Predicted Sale Price')\nplt.title(f'Figure {figure_number}: Lasso Residuals: {lasso.score(X_val,y_val)}', fontsize=20)\nplt.savefig(f'../images/figure_no_{figure_number}__{model_name}_residuals.png')\nplt.show();\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure = plt.figure(figsize=(10, 10))\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.ylabel('Residuals')\nplt.xlabel('Predicted Sale Price')\nplt.title(f'Figure {figure_number}: Lasso Residuals: {lasso.score(X_val,y_val)}', fontsize=20)\nplt.savefig(f'../images/figure_no_{figure_number}__{model_name}_residuals.png')\nplt.show();\n# %%\n# get the coefficients of the model as a dataframe",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 17\nfigure = plt.figure(figsize=(30, 30))\nmodel_name = 'Lasso'\ncoefficients = pd.DataFrame(lasso.coef_, X.columns, columns=[\"Coefficients\"])\n# %%\n# Inference variables\ncoefficients = coefficients # set to linear regression\nlargest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[0]\nlargest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure = plt.figure(figsize=(30, 30))\nmodel_name = 'Lasso'\ncoefficients = pd.DataFrame(lasso.coef_, X.columns, columns=[\"Coefficients\"])\n# %%\n# Inference variables\ncoefficients = coefficients # set to linear regression\nlargest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[0]\nlargest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_name = 'Lasso'\ncoefficients = pd.DataFrame(lasso.coef_, X.columns, columns=[\"Coefficients\"])\n# %%\n# Inference variables\ncoefficients = coefficients # set to linear regression\nlargest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[0]\nlargest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "coefficients",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "coefficients = pd.DataFrame(lasso.coef_, X.columns, columns=[\"Coefficients\"])\n# %%\n# Inference variables\ncoefficients = coefficients # set to linear regression\nlargest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[0]\nlargest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "coefficients",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "coefficients = coefficients # set to linear regression\nlargest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[0]\nlargest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_lasso = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_lasso = coefficients['Coefficients'].sort_values(ascending=False)[-4:]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "largest_coef_value",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "largest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[0]\nlargest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_lasso = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_lasso = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "largest_coef_feature",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "largest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_lasso = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_lasso = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\n# plot the coefficients",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "smallest_coef_value",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "smallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_lasso = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_lasso = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\n# plot the coefficients\nfigure_number = 18",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "smallest_coef_feature",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "smallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_lasso = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_lasso = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\n# plot the coefficients\nfigure_number = 18\ncoefficients[coefficients[\"Coefficients\"] > 0].sort_values(by=\"Coefficients\").plot(kind=\"barh\")",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "largest_coefs_four_lasso",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "largest_coefs_four_lasso = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_lasso = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\n# plot the coefficients\nfigure_number = 18\ncoefficients[coefficients[\"Coefficients\"] > 0].sort_values(by=\"Coefficients\").plot(kind=\"barh\")\n# spread out the y labels\nplt.yticks(rotation=0)\nplt.title(f'Figure No. {figure_number}: LR Positive Coefficients')\nplt.savefig(f\"../images/figure_no_{figure_number}_{model_name}_positivecoefficients.png\")",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "smallest_coefs_four_lasso",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "smallest_coefs_four_lasso = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\n# plot the coefficients\nfigure_number = 18\ncoefficients[coefficients[\"Coefficients\"] > 0].sort_values(by=\"Coefficients\").plot(kind=\"barh\")\n# spread out the y labels\nplt.yticks(rotation=0)\nplt.title(f'Figure No. {figure_number}: LR Positive Coefficients')\nplt.savefig(f\"../images/figure_no_{figure_number}_{model_name}_positivecoefficients.png\")\nplt.show();",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 18\ncoefficients[coefficients[\"Coefficients\"] > 0].sort_values(by=\"Coefficients\").plot(kind=\"barh\")\n# spread out the y labels\nplt.yticks(rotation=0)\nplt.title(f'Figure No. {figure_number}: LR Positive Coefficients')\nplt.savefig(f\"../images/figure_no_{figure_number}_{model_name}_positivecoefficients.png\")\nplt.show();\n# %%\n# add the model scores to the model_scores dataframe\nrow['model_name'] = 'Lasso'",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['model_name']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['model_name'] = 'Lasso'\nrow['r2_score'] = lasso_score\nrow['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['r2_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['r2_score'] = lasso_score\nrow['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %% [markdown]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['rmse_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %% [markdown]\n# # Ridge Regression Model",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['train_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %% [markdown]\n# # Ridge Regression Model\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['test_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %% [markdown]\n# # Ridge Regression Model\n# %%\n# Building our Ridge Model",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['cval_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %% [markdown]\n# # Ridge Regression Model\n# %%\n# Building our Ridge Model\nfrom sklearn.linear_model import Ridge",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row = pd.DataFrame(row, index=[0])\n# row format will be: model_name, r2 score, rmse score, train_score, test_score, train_rmse, test_rmse, crossval_score\n# %%\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %% [markdown]\n# # Ridge Regression Model\n# %%\n# Building our Ridge Model\nfrom sklearn.linear_model import Ridge\nridge = Ridge()",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_scores_df",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_scores_df = model_scores_df.append(row, ignore_index=True)\n# %% [markdown]\n# # Ridge Regression Model\n# %%\n# Building our Ridge Model\nfrom sklearn.linear_model import Ridge\nridge = Ridge()\nX = train.drop(columns=['saleprice'])\ny = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42) # split the data into train and validation sets",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "ridge",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "ridge = Ridge()\nX = train.drop(columns=['saleprice'])\ny = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42) # split the data into train and validation sets\nX_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nridge.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\ny_preds = ridge.predict(X_train) # predict on training data\ny_val_preds = ridge.predict(X_val) # predict on validation data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "X = train.drop(columns=['saleprice'])\ny = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42) # split the data into train and validation sets\nX_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nridge.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\ny_preds = ridge.predict(X_train) # predict on training data\ny_val_preds = ridge.predict(X_val) # predict on validation data\nridge.score(X_train, y_train) # score on training data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "y = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42) # split the data into train and validation sets\nX_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nridge.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\ny_preds = ridge.predict(X_train) # predict on training data\ny_val_preds = ridge.predict(X_val) # predict on validation data\nridge.score(X_train, y_train) # score on training data\nridge_score = ridge.score(X_val, y_val)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "X_train",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "X_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nridge.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\ny_preds = ridge.predict(X_train) # predict on training data\ny_val_preds = ridge.predict(X_val) # predict on validation data\nridge.score(X_train, y_train) # score on training data\nridge_score = ridge.score(X_val, y_val)\nprint(f'validataion score: {ridge.score(X_val, y_val)}') # score on validation data\nprint(f'training score: {ridge.score(X_train, y_train)}') # score on training data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "X_val",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "X_val = ss.transform(X_val)\nridge.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\ny_preds = ridge.predict(X_train) # predict on training data\ny_val_preds = ridge.predict(X_val) # predict on validation data\nridge.score(X_train, y_train) # score on training data\nridge_score = ridge.score(X_val, y_val)\nprint(f'validataion score: {ridge.score(X_val, y_val)}') # score on validation data\nprint(f'training score: {ridge.score(X_train, y_train)}') # score on training data\ntrain_score = ridge.score(X_train, y_train) # score on training data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "y_preds",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "y_preds = ridge.predict(X_train) # predict on training data\ny_val_preds = ridge.predict(X_val) # predict on validation data\nridge.score(X_train, y_train) # score on training data\nridge_score = ridge.score(X_val, y_val)\nprint(f'validataion score: {ridge.score(X_val, y_val)}') # score on validation data\nprint(f'training score: {ridge.score(X_train, y_train)}') # score on training data\ntrain_score = ridge.score(X_train, y_train) # score on training data\ntest_score = ridge.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(ridge, X_train, y_train, cv=5).mean() # score on cross validation data\nprint(f'cross_val_score (train): {cross_val_score(ridge, X_train, y_train, cv=5).mean()}') # cross_val_score on training data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "y_val_preds",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "y_val_preds = ridge.predict(X_val) # predict on validation data\nridge.score(X_train, y_train) # score on training data\nridge_score = ridge.score(X_val, y_val)\nprint(f'validataion score: {ridge.score(X_val, y_val)}') # score on validation data\nprint(f'training score: {ridge.score(X_train, y_train)}') # score on training data\ntrain_score = ridge.score(X_train, y_train) # score on training data\ntest_score = ridge.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(ridge, X_train, y_train, cv=5).mean() # score on cross validation data\nprint(f'cross_val_score (train): {cross_val_score(ridge, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\nprint(f'cross_val_score (validation): {cross_val_score(ridge, X_train, y_train, cv=5).mean()}') # cross_val_score on training data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "ridge_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "ridge_score = ridge.score(X_val, y_val)\nprint(f'validataion score: {ridge.score(X_val, y_val)}') # score on validation data\nprint(f'training score: {ridge.score(X_train, y_train)}') # score on training data\ntrain_score = ridge.score(X_train, y_train) # score on training data\ntest_score = ridge.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(ridge, X_train, y_train, cv=5).mean() # score on cross validation data\nprint(f'cross_val_score (train): {cross_val_score(ridge, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\nprint(f'cross_val_score (validation): {cross_val_score(ridge, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\n# add the score to the model_scores dictionary\nmodel_scores['ridge'] = ridge.score(X_val, y_val)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train_score = ridge.score(X_train, y_train) # score on training data\ntest_score = ridge.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(ridge, X_train, y_train, cv=5).mean() # score on cross validation data\nprint(f'cross_val_score (train): {cross_val_score(ridge, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\nprint(f'cross_val_score (validation): {cross_val_score(ridge, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\n# add the score to the model_scores dictionary\nmodel_scores['ridge'] = ridge.score(X_val, y_val)\n# add the predictions to the model_predictions dictionary\nmodel_predictions['ridge'] = y_val_preds\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test_score = ridge.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(ridge, X_train, y_train, cv=5).mean() # score on cross validation data\nprint(f'cross_val_score (train): {cross_val_score(ridge, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\nprint(f'cross_val_score (validation): {cross_val_score(ridge, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\n# add the score to the model_scores dictionary\nmodel_scores['ridge'] = ridge.score(X_val, y_val)\n# add the predictions to the model_predictions dictionary\nmodel_predictions['ridge'] = y_val_preds\n# %%\nrow['model_name'] = 'Ridge'",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "cval_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "cval_score = cross_val_score(ridge, X_train, y_train, cv=5).mean() # score on cross validation data\nprint(f'cross_val_score (train): {cross_val_score(ridge, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\nprint(f'cross_val_score (validation): {cross_val_score(ridge, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\n# add the score to the model_scores dictionary\nmodel_scores['ridge'] = ridge.score(X_val, y_val)\n# add the predictions to the model_predictions dictionary\nmodel_predictions['ridge'] = y_val_preds\n# %%\nrow['model_name'] = 'Ridge'\nrow['r2_score'] = ridge.score(X_val, y_val)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_scores['ridge']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_scores['ridge'] = ridge.score(X_val, y_val)\n# add the predictions to the model_predictions dictionary\nmodel_predictions['ridge'] = y_val_preds\n# %%\nrow['model_name'] = 'Ridge'\nrow['r2_score'] = ridge.score(X_val, y_val)\nrow['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_predictions['ridge']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_predictions['ridge'] = y_val_preds\n# %%\nrow['model_name'] = 'Ridge'\nrow['r2_score'] = ridge.score(X_val, y_val)\nrow['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['model_name']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['model_name'] = 'Ridge'\nrow['r2_score'] = ridge.score(X_val, y_val)\nrow['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# Plot the predictions vs the actual values",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['r2_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['r2_score'] = ridge.score(X_val, y_val)\nrow['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 19",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['rmse_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 19\nmodel = 'Ridge'",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['train_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 19\nmodel = 'Ridge'\nplt.figure(figsize=(10, 10))",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['test_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 19\nmodel = 'Ridge'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['cval_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 19\nmodel = 'Ridge'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 19\nmodel = 'Ridge'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\nplt.xlabel('Actual Sale Price')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_scores_df",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# Plot the predictions vs the actual values\nfigure_number = 19\nmodel = 'Ridge'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 19\nmodel = 'Ridge'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\nplt.annotate(f'model: {model} \\ntrain:{train_score}\\ntest:{test_score}\\ncval:{cval_score}', xy=(0.05, 0.75), xycoords='axes fraction')\nplt.title(f'Figure {figure_number}: {model} Predicted vs Actual Sale Price')\nplt.savefig(f'../images/figure_no_{figure_number}_{model}_actual_vs_predicted.png')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model = 'Ridge'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\nplt.annotate(f'model: {model} \\ntrain:{train_score}\\ntest:{test_score}\\ncval:{cval_score}', xy=(0.05, 0.75), xycoords='axes fraction')\nplt.title(f'Figure {figure_number}: {model} Predicted vs Actual Sale Price')\nplt.savefig(f'../images/figure_no_{figure_number}_{model}_actual_vs_predicted.png')\nplt.show();",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 20\nmodel = 'Ridge'\nfigure = plt.figure(figsize=(10, 10))\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.title(f'Figure {figure_number}: {model} Residuals: { ridge.score(X_val, y_val)}', fontsize=20)\nplt.annotate(f'model: {model} \\nR2: {ridge_score}', xy=(0.1, 0.9), xycoords='axes fraction', fontsize=15)\nplt.ylabel('Residuals')\nplt.xlabel('Predicted Sale Price')\nplt.savefig(f'../images/figure_no_{figure_number}_{model}_residuals.png')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model = 'Ridge'\nfigure = plt.figure(figsize=(10, 10))\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.title(f'Figure {figure_number}: {model} Residuals: { ridge.score(X_val, y_val)}', fontsize=20)\nplt.annotate(f'model: {model} \\nR2: {ridge_score}', xy=(0.1, 0.9), xycoords='axes fraction', fontsize=15)\nplt.ylabel('Residuals')\nplt.xlabel('Predicted Sale Price')\nplt.savefig(f'../images/figure_no_{figure_number}_{model}_residuals.png')\nridge_score = ridge.score(X_val, y_val)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure = plt.figure(figsize=(10, 10))\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.title(f'Figure {figure_number}: {model} Residuals: { ridge.score(X_val, y_val)}', fontsize=20)\nplt.annotate(f'model: {model} \\nR2: {ridge_score}', xy=(0.1, 0.9), xycoords='axes fraction', fontsize=15)\nplt.ylabel('Residuals')\nplt.xlabel('Predicted Sale Price')\nplt.savefig(f'../images/figure_no_{figure_number}_{model}_residuals.png')\nridge_score = ridge.score(X_val, y_val)\nplt.show();",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "ridge_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "ridge_score = ridge.score(X_val, y_val)\nplt.show();\n# %%\n# get the coefficients of the model as a dataframe\nfigure_number = 21\nmodel_name = 'Ridge'\ncoefficients = pd.DataFrame(ridge.coef_, X.columns, columns=[\"Coefficients\"])\ncoefficients.head()\n# %%\n# plot the coefficients",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 21\nmodel_name = 'Ridge'\ncoefficients = pd.DataFrame(ridge.coef_, X.columns, columns=[\"Coefficients\"])\ncoefficients.head()\n# %%\n# plot the coefficients\ncoefficients[coefficients[\"Coefficients\"] > 0].sort_values(by=\"Coefficients\").plot(kind=\"barh\")\n# spread out the y labels\nplt.yticks(rotation=0)\nplt.title(f'Figure No. {figure_number}: {model_name} Positive Coefficients {ridge.score(X_val, y_val)}')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_name = 'Ridge'\ncoefficients = pd.DataFrame(ridge.coef_, X.columns, columns=[\"Coefficients\"])\ncoefficients.head()\n# %%\n# plot the coefficients\ncoefficients[coefficients[\"Coefficients\"] > 0].sort_values(by=\"Coefficients\").plot(kind=\"barh\")\n# spread out the y labels\nplt.yticks(rotation=0)\nplt.title(f'Figure No. {figure_number}: {model_name} Positive Coefficients {ridge.score(X_val, y_val)}')\nplt.savefig(f\"../images/figure_no_{figure_number}_{model_name}_positivecoefficients.png\")",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "coefficients",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "coefficients = pd.DataFrame(ridge.coef_, X.columns, columns=[\"Coefficients\"])\ncoefficients.head()\n# %%\n# plot the coefficients\ncoefficients[coefficients[\"Coefficients\"] > 0].sort_values(by=\"Coefficients\").plot(kind=\"barh\")\n# spread out the y labels\nplt.yticks(rotation=0)\nplt.title(f'Figure No. {figure_number}: {model_name} Positive Coefficients {ridge.score(X_val, y_val)}')\nplt.savefig(f\"../images/figure_no_{figure_number}_{model_name}_positivecoefficients.png\")\nplt.show();",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "coefficients",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "coefficients = coefficients # set to linear regression\nlargest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[0]\nlargest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_ridge = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_ridge = coefficients['Coefficients'].sort_values(ascending=False)[-4:]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "largest_coef_value",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "largest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[0]\nlargest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_ridge = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_ridge = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "largest_coef_feature",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "largest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_ridge = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_ridge = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\nlr.coef_",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "smallest_coef_value",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "smallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_ridge = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_ridge = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\nlr.coef_\n# %% [markdown]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "smallest_coef_feature",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "smallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_ridge = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_ridge = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\nlr.coef_\n# %% [markdown]\n# # Elastic Net Model",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "largest_coefs_four_ridge",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "largest_coefs_four_ridge = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_ridge = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\nlr.coef_\n# %% [markdown]\n# # Elastic Net Model\n# %%\n# Building our ElasticNet Model\nfrom sklearn.linear_model import ElasticNet\nelasticnet = ElasticNet()",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "smallest_coefs_four_ridge",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "smallest_coefs_four_ridge = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %%\nlr.coef_\n# %% [markdown]\n# # Elastic Net Model\n# %%\n# Building our ElasticNet Model\nfrom sklearn.linear_model import ElasticNet\nelasticnet = ElasticNet()\nmodel = 'ElasticNet'",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "elasticnet",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "elasticnet = ElasticNet()\nmodel = 'ElasticNet'\nX = train.drop(columns=['saleprice'])\ny = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42) # split the data into train and validation sets\nX_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nelasticnet.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\ny_preds = elasticnet.predict(X_train) # predict on training data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model = 'ElasticNet'\nX = train.drop(columns=['saleprice'])\ny = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42) # split the data into train and validation sets\nX_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nelasticnet.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\ny_preds = elasticnet.predict(X_train) # predict on training data\ny_val_preds = elasticnet.predict(X_val) # predict on validation data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "X = train.drop(columns=['saleprice'])\ny = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42) # split the data into train and validation sets\nX_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nelasticnet.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\ny_preds = elasticnet.predict(X_train) # predict on training data\ny_val_preds = elasticnet.predict(X_val) # predict on validation data\nelasticnet.score(X_train, y_train) # score on training data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "y = train['saleprice'] # target\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42) # split the data into train and validation sets\nX_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nelasticnet.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\ny_preds = elasticnet.predict(X_train) # predict on training data\ny_val_preds = elasticnet.predict(X_val) # predict on validation data\nelasticnet.score(X_train, y_train) # score on training data\nprint(f'validataion score: {elasticnet.score(X_val, y_val)}') # score on validation data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "X_train",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "X_train = ss.fit_transform(X_train)\nX_val = ss.transform(X_val)\nelasticnet.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\ny_preds = elasticnet.predict(X_train) # predict on training data\ny_val_preds = elasticnet.predict(X_val) # predict on validation data\nelasticnet.score(X_train, y_train) # score on training data\nprint(f'validataion score: {elasticnet.score(X_val, y_val)}') # score on validation data\nprint(f'training score: {elasticnet.score(X_train, y_train)}') # score on training data\nprint(f'cross_val_score (train): {cross_val_score(elasticnet, X_train, y_train, cv=5).mean()}') # cross_val_score on training data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "X_val",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "X_val = ss.transform(X_val)\nelasticnet.fit(X_train,y_train) # fit on training data\n# Predictions and Evaluation of the Model\ny_preds = elasticnet.predict(X_train) # predict on training data\ny_val_preds = elasticnet.predict(X_val) # predict on validation data\nelasticnet.score(X_train, y_train) # score on training data\nprint(f'validataion score: {elasticnet.score(X_val, y_val)}') # score on validation data\nprint(f'training score: {elasticnet.score(X_train, y_train)}') # score on training data\nprint(f'cross_val_score (train): {cross_val_score(elasticnet, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\nprint(f'cross_val_score (validation): {cross_val_score(elasticnet, X_train, y_train, cv=5).mean()}') # cross_val_score on training data",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "y_preds",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "y_preds = elasticnet.predict(X_train) # predict on training data\ny_val_preds = elasticnet.predict(X_val) # predict on validation data\nelasticnet.score(X_train, y_train) # score on training data\nprint(f'validataion score: {elasticnet.score(X_val, y_val)}') # score on validation data\nprint(f'training score: {elasticnet.score(X_train, y_train)}') # score on training data\nprint(f'cross_val_score (train): {cross_val_score(elasticnet, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\nprint(f'cross_val_score (validation): {cross_val_score(elasticnet, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\n# add the score to the model_scores dictionary\nmodel_scores['elasticnet'] = elasticnet.score(X_val, y_val)\n# add the predictions to the model_predictions dictionary",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "y_val_preds",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "y_val_preds = elasticnet.predict(X_val) # predict on validation data\nelasticnet.score(X_train, y_train) # score on training data\nprint(f'validataion score: {elasticnet.score(X_val, y_val)}') # score on validation data\nprint(f'training score: {elasticnet.score(X_train, y_train)}') # score on training data\nprint(f'cross_val_score (train): {cross_val_score(elasticnet, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\nprint(f'cross_val_score (validation): {cross_val_score(elasticnet, X_train, y_train, cv=5).mean()}') # cross_val_score on training data\n# add the score to the model_scores dictionary\nmodel_scores['elasticnet'] = elasticnet.score(X_val, y_val)\n# add the predictions to the model_predictions dictionary\nmodel_predictions['elasticnet'] = y_val_preds",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_scores['elasticnet']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_scores['elasticnet'] = elasticnet.score(X_val, y_val)\n# add the predictions to the model_predictions dictionary\nmodel_predictions['elasticnet'] = y_val_preds\ntrain_score = elasticnet.score(X_train, y_train) # score on training data\ntest_score = elasticnet.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(elasticnet, X_train, y_train, cv=5).mean() # score on cross validation data\n# Plot the predictions vs the actual values\nfigure_number = 23\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_predictions['elasticnet']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_predictions['elasticnet'] = y_val_preds\ntrain_score = elasticnet.score(X_train, y_train) # score on training data\ntest_score = elasticnet.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(elasticnet, X_train, y_train, cv=5).mean() # score on cross validation data\n# Plot the predictions vs the actual values\nfigure_number = 23\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\nplt.xlabel('Actual Sale Price')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "train_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "train_score = elasticnet.score(X_train, y_train) # score on training data\ntest_score = elasticnet.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(elasticnet, X_train, y_train, cv=5).mean() # score on cross validation data\n# Plot the predictions vs the actual values\nfigure_number = 23\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "test_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "test_score = elasticnet.score(X_val, y_val) # score on validation data\ncval_score = cross_val_score(elasticnet, X_train, y_train, cv=5).mean() # score on cross validation data\n# Plot the predictions vs the actual values\nfigure_number = 23\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\nplt.annotate(f'model: {model} \\ntrain:{train_score}\\ntest:{test_score}\\ncval:{cval_score}', xy=(0.05, 0.75), xycoords='axes fraction')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "cval_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "cval_score = cross_val_score(elasticnet, X_train, y_train, cv=5).mean() # score on cross validation data\n# Plot the predictions vs the actual values\nfigure_number = 23\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\nplt.annotate(f'model: {model} \\ntrain:{train_score}\\ntest:{test_score}\\ncval:{cval_score}', xy=(0.05, 0.75), xycoords='axes fraction')\nplt.title(f'Figure {figure_number}: {model} Predicted vs Actual Sale Price', fontsize=15)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 23\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, y_val_preds) # plot the predictions vs the actual values\nplt.plot([0, 1], [0, 1], transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\nplt.annotate(f'model: {model} \\ntrain:{train_score}\\ntest:{test_score}\\ncval:{cval_score}', xy=(0.05, 0.75), xycoords='axes fraction')\nplt.title(f'Figure {figure_number}: {model} Predicted vs Actual Sale Price', fontsize=15)\nplt.savefig(f'../images/figure_no_{figure_number}_{model}_actual_vs_predicted.png')\nelasticnet_score = elasticnet.score(X_val, y_val)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "elasticnet_score",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "elasticnet_score = elasticnet.score(X_val, y_val)\nplt.show();\n# %%\nrow['model_name'] = 'ElasticNet'\nrow['r2_score'] = elasticnet_score\nrow['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['model_name']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['model_name'] = 'ElasticNet'\nrow['r2_score'] = elasticnet_score\nrow['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# plot the residuals",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['r2_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['r2_score'] = elasticnet_score\nrow['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# plot the residuals\nfigure_number = 24",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['rmse_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['rmse_score'] = np.sqrt(mean_squared_error(y_val, y_val_preds))\nrow['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# plot the residuals\nfigure_number = 24\nfigure = plt.figure(figsize=(10, 10))",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['train_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['train_score'] = train_score\nrow['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# plot the residuals\nfigure_number = 24\nfigure = plt.figure(figsize=(10, 10))\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['test_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['test_score'] = test_score\nrow['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# plot the residuals\nfigure_number = 24\nfigure = plt.figure(figsize=(10, 10))\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.title(f'Figure {figure_number}: ElasticNet Residuals', fontsize=15)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row['cval_score']",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row['cval_score'] = cval_score\nrow = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# plot the residuals\nfigure_number = 24\nfigure = plt.figure(figsize=(10, 10))\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.title(f'Figure {figure_number}: ElasticNet Residuals', fontsize=15)\nplt.annotate(f'model: {model} \\ntrain:{train_score}\\ntest:{test_score}\\ncval:{cval_score}', xy=(0.05, 0.75), xycoords='axes fraction')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "row",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "row = pd.DataFrame(row, index=[0])\nmodel_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# plot the residuals\nfigure_number = 24\nfigure = plt.figure(figsize=(10, 10))\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.title(f'Figure {figure_number}: ElasticNet Residuals', fontsize=15)\nplt.annotate(f'model: {model} \\ntrain:{train_score}\\ntest:{test_score}\\ncval:{cval_score}', xy=(0.05, 0.75), xycoords='axes fraction')\nplt.ylabel('Residuals')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_scores_df",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_scores_df = model_scores_df.append(row, ignore_index=True)\n# %%\n# plot the residuals\nfigure_number = 24\nfigure = plt.figure(figsize=(10, 10))\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.title(f'Figure {figure_number}: ElasticNet Residuals', fontsize=15)\nplt.annotate(f'model: {model} \\ntrain:{train_score}\\ntest:{test_score}\\ncval:{cval_score}', xy=(0.05, 0.75), xycoords='axes fraction')\nplt.ylabel('Residuals')\nplt.xlabel('Predicted Sale Price')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 24\nfigure = plt.figure(figsize=(10, 10))\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.title(f'Figure {figure_number}: ElasticNet Residuals', fontsize=15)\nplt.annotate(f'model: {model} \\ntrain:{train_score}\\ntest:{test_score}\\ncval:{cval_score}', xy=(0.05, 0.75), xycoords='axes fraction')\nplt.ylabel('Residuals')\nplt.xlabel('Predicted Sale Price')\nplt.savefig(f'../images/figure_no_{figure_number}_{model}.png')\nplt.show();\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure = plt.figure(figsize=(10, 10))\nsns.residplot(y_val_preds, y_val, lowess=True, color=\"g\")\nplt.title(f'Figure {figure_number}: ElasticNet Residuals', fontsize=15)\nplt.annotate(f'model: {model} \\ntrain:{train_score}\\ntest:{test_score}\\ncval:{cval_score}', xy=(0.05, 0.75), xycoords='axes fraction')\nplt.ylabel('Residuals')\nplt.xlabel('Predicted Sale Price')\nplt.savefig(f'../images/figure_no_{figure_number}_{model}.png')\nplt.show();\n# %%\n# get the coefficients of the model as a dataframe",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "coefficients",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "coefficients = pd.DataFrame(elasticnet.coef_, X.columns, columns=[\"Coefficients\"])\n# Inference variables\ncoefficients = coefficients # set to linear regression\nlargest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[0]\nlargest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "coefficients",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "coefficients = coefficients # set to linear regression\nlargest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[0]\nlargest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_elasticnet = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_elasticnet = coefficients['Coefficients'].sort_values(ascending=False)[-4:]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "largest_coef_value",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "largest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[0]\nlargest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_elasticnet = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_elasticnet = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %% [markdown]",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "largest_coef_feature",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "largest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[0]\nsmallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_elasticnet = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_elasticnet = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %% [markdown]\n# Differences between Lasso and Ridge Models:",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "smallest_coef_value",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "smallest_coef_value = coefficients['Coefficients'].sort_values(ascending=False)[-1]\nsmallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_elasticnet = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_elasticnet = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %% [markdown]\n# Differences between Lasso and Ridge Models:\n# * Ridge: It includes all (or none) of the features in the model. Thus, the major advantage of ridge regression is coefficient shrinkage and reducing model complexity. It is majorly used to prevent overfitting. Since it includes all the features, it is not very useful in case of exorbitantly high #features, say in millions, as it will pose computational challenges.",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "smallest_coef_feature",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "smallest_coef_feature = coefficients['Coefficients'].sort_values(ascending=False).index[-1]\n# Inference on the coefficients\nprint(f'A one unit increase in {largest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[largest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.') # log unit increase, because we are using regularized regression\nprint(f'A one unit decrease in {smallest_coef_feature}, holding all other features constant, will result in a {coefficients.loc[smallest_coef_feature, \"Coefficients\"]} log-unit increase in saleprice.')\nlargest_coefs_four_elasticnet = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_elasticnet = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %% [markdown]\n# Differences between Lasso and Ridge Models:\n# * Ridge: It includes all (or none) of the features in the model. Thus, the major advantage of ridge regression is coefficient shrinkage and reducing model complexity. It is majorly used to prevent overfitting. Since it includes all the features, it is not very useful in case of exorbitantly high #features, say in millions, as it will pose computational challenges.\n# * Lasso: Along with shrinking coefficients, lasso performs feature selection as well. (Remember the ‘selection‘ in the lasso full-form?) As we observed earlier, some of the coefficients become exactly zero, which is equivalent to the particular feature being excluded from the model. Since it provides sparse solutions, it is generally the model of choice (or some variant of this concept) for modelling cases where the #features are in millions or more. In such a case, getting a sparse solution is of great computational advantage as the features with zero coefficients can simply be ignored.",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "largest_coefs_four_elasticnet",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "largest_coefs_four_elasticnet = coefficients['Coefficients'].sort_values(ascending=False)[0:4]\nsmallest_coefs_four_elasticnet = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %% [markdown]\n# Differences between Lasso and Ridge Models:\n# * Ridge: It includes all (or none) of the features in the model. Thus, the major advantage of ridge regression is coefficient shrinkage and reducing model complexity. It is majorly used to prevent overfitting. Since it includes all the features, it is not very useful in case of exorbitantly high #features, say in millions, as it will pose computational challenges.\n# * Lasso: Along with shrinking coefficients, lasso performs feature selection as well. (Remember the ‘selection‘ in the lasso full-form?) As we observed earlier, some of the coefficients become exactly zero, which is equivalent to the particular feature being excluded from the model. Since it provides sparse solutions, it is generally the model of choice (or some variant of this concept) for modelling cases where the #features are in millions or more. In such a case, getting a sparse solution is of great computational advantage as the features with zero coefficients can simply be ignored.\n#\n# source: https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/\n# %% [markdown]\n# Now, I want to display the most impactful features in the models that I have created after evaluating which of the models is the best fit for our scenario where we have a R2 goal of at least 0.8 and a RMSE goal of less than 40,000.",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "smallest_coefs_four_elasticnet",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "smallest_coefs_four_elasticnet = coefficients['Coefficients'].sort_values(ascending=False)[-4:]\n# %% [markdown]\n# Differences between Lasso and Ridge Models:\n# * Ridge: It includes all (or none) of the features in the model. Thus, the major advantage of ridge regression is coefficient shrinkage and reducing model complexity. It is majorly used to prevent overfitting. Since it includes all the features, it is not very useful in case of exorbitantly high #features, say in millions, as it will pose computational challenges.\n# * Lasso: Along with shrinking coefficients, lasso performs feature selection as well. (Remember the ‘selection‘ in the lasso full-form?) As we observed earlier, some of the coefficients become exactly zero, which is equivalent to the particular feature being excluded from the model. Since it provides sparse solutions, it is generally the model of choice (or some variant of this concept) for modelling cases where the #features are in millions or more. In such a case, getting a sparse solution is of great computational advantage as the features with zero coefficients can simply be ignored.\n#\n# source: https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/\n# %% [markdown]\n# Now, I want to display the most impactful features in the models that I have created after evaluating which of the models is the best fit for our scenario where we have a R2 goal of at least 0.8 and a RMSE goal of less than 40,000.\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 25\nfigure = plt.figure(figsize=(10, 10))\nsns.heatmap(pd.DataFrame({'LR': lr.coef_, 'LASSO': lasso.coef_, 'RIDGE': ridge.coef_, 'ELASTICNET': elasticnet.coef_}, index=X.columns), annot=True, cmap='coolwarm', fmt='.2f', linewidths=1, linecolor='black')\nplt.title(f'Figure {figure_number}: Coefficient Values for LR, LASSO, RIDGE, and ELASTICNET')\nplt.savefig(f'../images/figure_no_{figure_number}_coefficient_values_for_LR_LASSO_RIDGE_and_ELASTICNET.png')\nplt.xlabel('Models')\nplt.ylabel('Features')\nplt.show();\n# %% [markdown]\n# When interpreting this figure, consider that Lasso, Ridge, and ElasticNet are all regularized models. This means that they are penalized for having too many features. When we interpret these coefficients we say that changes in the feature result in a increase or decrease in the log-odds rather than the odds themselves.",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure = plt.figure(figsize=(10, 10))\nsns.heatmap(pd.DataFrame({'LR': lr.coef_, 'LASSO': lasso.coef_, 'RIDGE': ridge.coef_, 'ELASTICNET': elasticnet.coef_}, index=X.columns), annot=True, cmap='coolwarm', fmt='.2f', linewidths=1, linecolor='black')\nplt.title(f'Figure {figure_number}: Coefficient Values for LR, LASSO, RIDGE, and ELASTICNET')\nplt.savefig(f'../images/figure_no_{figure_number}_coefficient_values_for_LR_LASSO_RIDGE_and_ELASTICNET.png')\nplt.xlabel('Models')\nplt.ylabel('Features')\nplt.show();\n# %% [markdown]\n# When interpreting this figure, consider that Lasso, Ridge, and ElasticNet are all regularized models. This means that they are penalized for having too many features. When we interpret these coefficients we say that changes in the feature result in a increase or decrease in the log-odds rather than the odds themselves.\n#",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "data = pd.DataFrame.from_dict(model_scores, orient='index').T\n# %%\ndata.head()\n# %%\nfigure_number = 26\ndata.plot(kind='bar', figsize=(10, 5), title=f'Figure {figure_number}: Model Scores')\n# %%\nmodel_predictions = pd.DataFrame(model_predictions)\nmodel_predictions.head()\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 26\ndata.plot(kind='bar', figsize=(10, 5), title=f'Figure {figure_number}: Model Scores')\n# %%\nmodel_predictions = pd.DataFrame(model_predictions)\nmodel_predictions.head()\n# %%\n# combine model_predictions and model_scores into a single dataframe with a added column for the model name.\nmodel_df = pd.DataFrame()\nfor model in model_predictions.columns:\n    row = {'model': model, 'predictions': model_predictions[model].to_list(), 'score': model_scores[m",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_predictions",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_predictions = pd.DataFrame(model_predictions)\nmodel_predictions.head()\n# %%\n# combine model_predictions and model_scores into a single dataframe with a added column for the model name.\nmodel_df = pd.DataFrame()\nfor model in model_predictions.columns:\n    row = {'model': model, 'predictions': model_predictions[model].to_list(), 'score': model_scores[m",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_df",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_df = pd.DataFrame()\nfor model in model_predictions.columns:\n    row = {'model': model, 'predictions': model_predictions[model].to_list(), 'score': model_scores[m\n    odel]}\n    model_df = model_df.append(row, ignore_index=True)\nmodel_df.head()\n# %%",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 27\nmodel_name = 'LinearRegression'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, model_df['predictions'][0], alpha = 0.5) # plot the predictions vs the actual values for the first model lr\nplt.scatter(y_val, model_df['predictions'][1],alpha = 0.5) # plot the predictions vs the actual values for lasso\nplt.scatter(y_val, model_df['predictions'][2],alpha = 0.5) # plot the predictions vs the actual values for ridge\nplt.scatter(y_val, model_df['predictions'][3],alpha = 0.5) # plot the predictions vs the actual values for the elasticnet model\nplt.plot([0, 1], [0, 1],\n         transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\nplt.xlabel('Actual Sale Price')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "model_name = 'LinearRegression'\nplt.figure(figsize=(10, 10))\nplt.scatter(y_val, model_df['predictions'][0], alpha = 0.5) # plot the predictions vs the actual values for the first model lr\nplt.scatter(y_val, model_df['predictions'][1],alpha = 0.5) # plot the predictions vs the actual values for lasso\nplt.scatter(y_val, model_df['predictions'][2],alpha = 0.5) # plot the predictions vs the actual values for ridge\nplt.scatter(y_val, model_df['predictions'][3],alpha = 0.5) # plot the predictions vs the actual values for the elasticnet model\nplt.plot([0, 1], [0, 1],\n         transform=plt.gca().transAxes, ls=\"--\", c=\".3\")\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "df = pd.DataFrame({'lr': lr.coef_, 'lasso': lasso.coef_, 'ridge': ridge.coef_, 'elasticnet': elasticnet.coef_}, index=X.columns)\nfigure_number = 28# plot the residuals\nfig, axes = plt.subplots(nrows=1, ncols=4, figsize=(30,5))\naxes[0].scatter(y_val, lr.predict(X_val_lr))\naxes[1].scatter(y_val, lasso.predict(X_val))\naxes[2].scatter(y_val, ridge.predict(X_val))\naxes[3].scatter(y_val, elasticnet.predict(X_val))\nplt.suptitle(f'Figure {figure_number}. Comparison of the 4 models',fontsize = 20)\naxes[0].set_xlabel(\"Actual Prices\") # set the x label\naxes[0].set_ylabel(\"Predicted Prices\") # set the y label",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "figure_number",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "figure_number = 28# plot the residuals\nfig, axes = plt.subplots(nrows=1, ncols=4, figsize=(30,5))\naxes[0].scatter(y_val, lr.predict(X_val_lr))\naxes[1].scatter(y_val, lasso.predict(X_val))\naxes[2].scatter(y_val, ridge.predict(X_val))\naxes[3].scatter(y_val, elasticnet.predict(X_val))\nplt.suptitle(f'Figure {figure_number}. Comparison of the 4 models',fontsize = 20)\naxes[0].set_xlabel(\"Actual Prices\") # set the x label\naxes[0].set_ylabel(\"Predicted Prices\") # set the y label\naxes[0].set_title(\"Linear Regression\") # set the title",
        "detail": "scripts.main",
        "documentation": {}
    }
]